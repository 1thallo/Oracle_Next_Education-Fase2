{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fd1f825",
   "metadata": {},
   "source": [
    "# Challenge TelecomX - Parte 2: Modelos Preditivos\n",
    "\n",
    "## Objetivo\n",
    "Criar modelos de Machine Learning para prever a evasão de clientes (churn) da TelecomX, utilizando os dados já tratados na Parte 1 do desafio.\n",
    "\n",
    "## Etapas do Projeto\n",
    "1. **Carregamento dos dados tratados**\n",
    "2. **Análise exploratória para modelagem**\n",
    "3. **Preparação dos dados para ML**\n",
    "4. **Construção de modelos preditivos**\n",
    "5. **Avaliação e comparação dos modelos**\n",
    "6. **Interpretação dos resultados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b930d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFalha ao iniciar o Kernel. \n",
      "\u001b[1;31mNão é possível iniciar o Kernel \".venv (Python 3.13.2)\" devido a um tempo limite aguardando as portas serem usadas. \n",
      "\u001b[1;31mConsulte o <a href='command:jupyter.viewOutput'>log</a> do Jupyter para obter mais detalhes."
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn pandas numpy matplotlib seaborn imbalanced-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64acb290",
   "metadata": {},
   "source": [
    "## 1. Carregamento dos Dados Tratados\n",
    "\n",
    "Carregando o conjunto de dados que foi limpo e tratado na Parte 1 do Challenge TelecomX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28cd98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dados = pd.read_csv(\"dados_tratados.csv\")\n",
    "    print(\"Dados tratados carregados do arquivo CSV\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Arquivo dados_tratados.csv não encontrado\")\n",
    "    print(\"Carregando e tratando dados da API...\")\n",
    "    \n",
    "    import requests\n",
    "    \n",
    "    url = 'https://github.com/alura-cursos/challenge2-data-science/raw/refs/heads/main/TelecomX_Data.json'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    dados = pd.json_normalize(data)\n",
    "    dados.replace({'Yes': 1, 'No': 0}, inplace=True)\n",
    "    \n",
    "    colunas_numericas = ['account.Charges.Monthly', 'account.Charges.Total', 'customer.tenure']\n",
    "    for col in colunas_numericas:\n",
    "        if col in dados.columns:\n",
    "            dados[col] = pd.to_numeric(dados[col], errors='coerce')\n",
    "    \n",
    "    if 'account.Charges.Monthly' in dados.columns:\n",
    "        dados['Contas_Diarias'] = dados['account.Charges.Monthly'] / 30\n",
    "    \n",
    "    dados = dados[dados['Churn'].isin([0, 1])]\n",
    "    \n",
    "    print(\"Dados carregados e tratados da API\")\n",
    "\n",
    "print(f\"Dimensões do dataset: {dados.shape}\")\n",
    "print(f\"Colunas disponíveis: {len(dados.columns)}\")\n",
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bf7361",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Informações do dataset:\")\n",
    "print(f\"Registros: {len(dados):,}\")\n",
    "print(f\"Colunas: {dados.columns.tolist()}\")\n",
    "print(f\"\\nTipos de dados:\")\n",
    "print(dados.dtypes)\n",
    "print(f\"\\nValores ausentes:\")\n",
    "print(dados.isnull().sum().sum())\n",
    "\n",
    "if 'Churn' in dados.columns:\n",
    "    print(f\"\\nDistribuição do churn:\")\n",
    "    churn_counts = dados['Churn'].value_counts()\n",
    "    print(f\"Não cancelaram (0): {churn_counts[0]:,} ({churn_counts[0]/len(dados)*100:.1f}%)\")\n",
    "    print(f\"Cancelaram (1): {churn_counts[1]:,} ({churn_counts[1]/len(dados)*100:.1f}%)\")\n",
    "    \n",
    "    plt.figure(figsize=(6,4))\n",
    "    dados['Churn'].value_counts().plot(kind='bar', color=['lightblue', 'coral'])\n",
    "    plt.title('Distribuição da Variável Target (Churn)')\n",
    "    plt.xlabel('Churn')\n",
    "    plt.ylabel('Quantidade')\n",
    "    plt.xticks([0,1], ['Não Cancelou', 'Cancelou'], rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef76aee1",
   "metadata": {},
   "source": [
    "## 2. Pré-processamento Avançado dos Dados\n",
    "\n",
    "### Etapas do Pré-processamento:\n",
    "1. **Eliminação de colunas irrelevantes** (IDs, identificadores únicos)\n",
    "2. **Codificação de variáveis categóricas** (One-Hot Encoding)\n",
    "3. **Análise do balanceamento das classes**\n",
    "4. **Aplicação de técnicas de balanceamento** (SMOTE, undersampling)\n",
    "5. **Normalização/Padronização** conforme necessário para cada modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbaf5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa 1: Eliminação de Colunas Irrelevantes\n",
    "\n",
    "print(\"Análise das colunas do dataset:\")\n",
    "\n",
    "print(\"Colunas disponíveis:\")\n",
    "for i, col in enumerate(dados.columns):\n",
    "    print(f\"{i+1:2d}. {col}\")\n",
    "    \n",
    "print(f\"\\nShape original dos dados: {dados.shape}\")\n",
    "\n",
    "colunas_para_remover = []\n",
    "\n",
    "for col in dados.columns:\n",
    "    if 'id' in col.lower() or 'customerid' in col.lower():\n",
    "        colunas_para_remover.append(col)\n",
    "        print(f\"Coluna '{col}' identificada como ID\")\n",
    "    elif dados[col].nunique() == len(dados):\n",
    "        colunas_para_remover.append(col)\n",
    "        print(f\"Coluna '{col}' tem valores únicos (possível ID)\")\n",
    "\n",
    "if colunas_para_remover:\n",
    "    dados_limpos = dados.drop(columns=colunas_para_remover)\n",
    "    print(f\"Colunas removidas: {colunas_para_remover}\")\n",
    "    print(f\"Shape após remoção: {dados_limpos.shape}\")\n",
    "else:\n",
    "    dados_limpos = dados.copy()\n",
    "    print(\"Nenhuma coluna irrelevante identificada automaticamente\")\n",
    "\n",
    "print(\"\\nColunas mantidas:\")\n",
    "for col in dados_limpos.columns:\n",
    "    tipo_dado = dados_limpos[col].dtype\n",
    "    valores_unicos = dados_limpos[col].nunique()\n",
    "    print(f\"• {col:20s} | Tipo: {str(tipo_dado):10s} | Únicos: {valores_unicos:4d}\")\n",
    "\n",
    "print(f\"\\nDados prontos para próxima etapa: {dados_limpos.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f6cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa 2: Codificação de Variáveis Categóricas\n",
    "\n",
    "print(\"Transformando variáveis categóricas\")\n",
    "\n",
    "colunas_categoricas = []\n",
    "colunas_numericas = []\n",
    "coluna_target = None\n",
    "\n",
    "for col in dados_limpos.columns:\n",
    "    if 'churn' in col.lower() or 'evasao' in col.lower() or 'cancelou' in col.lower():\n",
    "        coluna_target = col\n",
    "        print(f\"Variável target identificada: '{col}'\")\n",
    "    elif dados_limpos[col].dtype == 'object' or dados_limpos[col].dtype.name == 'category':\n",
    "        colunas_categoricas.append(col)\n",
    "    elif pd.api.types.is_numeric_dtype(dados_limpos[col]):\n",
    "        colunas_numericas.append(col)\n",
    "\n",
    "print(f\"\\nResumo das variáveis:\")\n",
    "print(f\"• Categóricas: {len(colunas_categoricas)} - {colunas_categoricas}\")\n",
    "print(f\"• Numéricas: {len(colunas_numericas)} - {colunas_numericas}\")\n",
    "print(f\"• Target: {coluna_target}\")\n",
    "\n",
    "if coluna_target:\n",
    "    X = dados_limpos.drop(coluna_target, axis=1)\n",
    "    y = dados_limpos[coluna_target]\n",
    "else:\n",
    "    X = dados_limpos.iloc[:, :-1]\n",
    "    y = dados_limpos.iloc[:, -1]\n",
    "    coluna_target = dados_limpos.columns[-1]\n",
    "\n",
    "X_numericas = X.select_dtypes(include=[np.number])\n",
    "X_categoricas = X.select_dtypes(include=['object', 'category'])\n",
    "\n",
    "print(f\"\\nSeparação concluída:\")\n",
    "print(f\"X (features): {X.shape}\")\n",
    "print(f\"y (target): {y.shape}\")\n",
    "\n",
    "if not X_categoricas.empty:\n",
    "    print(f\"\\nAplicando One-Hot Encoding em {len(X_categoricas.columns)} variáveis categóricas...\")\n",
    "    \n",
    "    # Converter todas as colunas categóricas para string para evitar erro de tipos mistos\n",
    "    X_categoricas_str = X_categoricas.astype(str)\n",
    "    \n",
    "    encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "    X_categoricas_encoded = encoder.fit_transform(X_categoricas_str)\n",
    "    \n",
    "    nomes_encoded = encoder.get_feature_names_out(X_categoricas.columns)\n",
    "    X_categoricas_encoded = pd.DataFrame(X_categoricas_encoded, \n",
    "                                       columns=nomes_encoded, \n",
    "                                       index=X_categoricas.index)\n",
    "    \n",
    "    X_final = pd.concat([X_numericas, X_categoricas_encoded], axis=1)\n",
    "    \n",
    "    print(f\"One-Hot Encoding aplicado\")\n",
    "    print(f\"Shape antes: {X.shape} | Shape depois: {X_final.shape}\")\n",
    "    print(f\"Novas colunas criadas: {len(nomes_encoded)}\")\n",
    "    \n",
    "else:\n",
    "    X_final = X.copy()\n",
    "    print(\"Nenhuma variável categórica encontrada - dados já numéricos\")\n",
    "\n",
    "if y.dtype == 'object':\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    print(f\"\\nTarget '{coluna_target}' codificado:\")\n",
    "    for i, classe in enumerate(label_encoder.classes_):\n",
    "        print(f\"   • {classe} → {i}\")\n",
    "else:\n",
    "    y_encoded = y.copy()\n",
    "    print(f\"\\nTarget '{coluna_target}' já está em formato numérico\")\n",
    "\n",
    "print(f\"\\nDados finais para modelagem:\")\n",
    "print(f\"• Features (X): {X_final.shape}\")\n",
    "print(f\"• Target (y): {y_encoded.shape}\")\n",
    "print(f\"• Colunas finais: {list(X_final.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4fc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa 3: Análise do Balanceamento das Classes\n",
    "\n",
    "print(\"Análise de balanceamento das classes\")\n",
    "\n",
    "distribuicao_classes = pd.Series(y_encoded).value_counts().sort_index()\n",
    "total_amostras = len(y_encoded)\n",
    "\n",
    "print(\"Distribuição absoluta:\")\n",
    "for classe, quantidade in distribuicao_classes.items():\n",
    "    print(f\"   Classe {classe}: {quantidade:,} amostras\")\n",
    "\n",
    "print(f\"\\nDistribuição percentual:\")\n",
    "for classe, quantidade in distribuicao_classes.items():\n",
    "    percentual = (quantidade / total_amostras) * 100\n",
    "    print(f\"   Classe {classe}: {percentual:.2f}%\")\n",
    "\n",
    "classe_majoritaria = distribuicao_classes.max()\n",
    "classe_minoritaria = distribuicao_classes.min()\n",
    "razao_desbalanceamento = classe_majoritaria / classe_minoritaria\n",
    "\n",
    "print(f\"\\nMétricas de balanceamento:\")\n",
    "print(f\"   • Classe majoritária: {classe_majoritaria:,} amostras\")\n",
    "print(f\"   • Classe minoritária: {classe_minoritaria:,} amostras\")\n",
    "print(f\"   • Razão de desbalanceamento: {razao_desbalanceamento:.2f}:1\")\n",
    "\n",
    "if razao_desbalanceamento <= 1.5:\n",
    "    nivel_desbalanceamento = \"BAIXO\"\n",
    "elif razao_desbalanceamento <= 3.0:\n",
    "    nivel_desbalanceamento = \"MODERADO\"\n",
    "elif razao_desbalanceamento <= 10.0:\n",
    "    nivel_desbalanceamento = \"ALTO\"\n",
    "else:\n",
    "    nivel_desbalanceamento = \"MUITO ALTO\"\n",
    "\n",
    "print(f\"   • Nível de desbalanceamento: {nivel_desbalanceamento}\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "distribuicao_classes.plot(kind='bar', color=['skyblue', 'lightcoral'])\n",
    "plt.title('Distribuição das Classes', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Classe', fontsize=12)\n",
    "plt.ylabel('Quantidade de Amostras', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "for i, v in enumerate(distribuicao_classes.values):\n",
    "    plt.text(i, v + total_amostras*0.01, f'{v:,}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "labels = [f'Classe {i}\\n({v:,} amostras)' for i, v in enumerate(distribuicao_classes.values)]\n",
    "colors = ['lightblue', 'lightcoral']\n",
    "plt.pie(distribuicao_classes.values, labels=labels, autopct='%1.1f%%', \n",
    "        colors=colors, startangle=90, textprops={'fontsize': 10})\n",
    "plt.title('Proporção das Classes', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nRecomendações para {nivel_desbalanceamento} desbalanceamento:\")\n",
    "if razao_desbalanceamento <= 1.5:\n",
    "    print(\"   Dataset bem balanceado - modelos podem ser treinados sem ajustes especiais\")\n",
    "elif razao_desbalanceamento <= 3.0:\n",
    "    print(\"   Considerar ajuste de class_weight nos modelos\")\n",
    "    print(\"   Monitorar métricas de precision e recall por classe\")\n",
    "elif razao_desbalanceamento <= 10.0:\n",
    "    print(\"   Recomendado aplicar técnicas de balanceamento (SMOTE, undersampling)\")\n",
    "    print(\"   Usar métricas balanceadas (F1-score, ROC-AUC) para avaliação\")\n",
    "    print(\"   Ajustar threshold de classificação se necessário\")\n",
    "else:\n",
    "    print(\"   OBRIGATÓRIO aplicar técnicas de balanceamento\")\n",
    "    print(\"   Considerar estratégias ensemble especializadas\")\n",
    "    print(\"   Foco em recall da classe minoritária\")\n",
    "\n",
    "print(f\"\\nAnálise de balanceamento concluída\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f07972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa 4: Aplicação de Técnicas de Balanceamento\n",
    "\n",
    "print(\"Aplicando técnicas de balanceamento\")\n",
    "\n",
    "datasets_balanceados = {}\n",
    "datasets_balanceados['Original'] = (X_final, y_encoded)\n",
    "\n",
    "if razao_desbalanceamento > 1.5:\n",
    "    print(f\"Aplicando técnicas de balanceamento (razão: {razao_desbalanceamento:.2f}:1)\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nAplicando SMOTE...\")\n",
    "        smote = SMOTE(random_state=42, k_neighbors=min(5, classe_minoritaria-1))\n",
    "        X_smote, y_smote = smote.fit_resample(X_final, y_encoded)\n",
    "        datasets_balanceados['SMOTE'] = (X_smote, y_smote)\n",
    "        print(f\"   SMOTE aplicado - Shape: {X_smote.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Erro no SMOTE: {e}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nAplicando Random Under Sampling...\")\n",
    "        undersampler = RandomUnderSampler(random_state=42)\n",
    "        X_under, y_under = undersampler.fit_resample(X_final, y_encoded)\n",
    "        datasets_balanceados['UnderSampling'] = (X_under, y_under)\n",
    "        print(f\"   Under Sampling aplicado - Shape: {X_under.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Erro no Under Sampling: {e}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nAplicando SMOTE + Tomek...\")\n",
    "        smote_tomek = SMOTETomek(random_state=42)\n",
    "        X_smote_tomek, y_smote_tomek = smote_tomek.fit_resample(X_final, y_encoded)\n",
    "        datasets_balanceados['SMOTE_Tomek'] = (X_smote_tomek, y_smote_tomek)\n",
    "        print(f\"   SMOTE+Tomek aplicado - Shape: {X_smote_tomek.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Erro no SMOTE+Tomek: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Dataset já bem balanceado (razão: {razao_desbalanceamento:.2f}:1) - balanceamento não necessário\")\n",
    "\n",
    "print(f\"\\nComparação das distribuições:\")\n",
    "\n",
    "for nome, (X_data, y_data) in datasets_balanceados.items():\n",
    "    distribuicao = pd.Series(y_data).value_counts().sort_index()\n",
    "    total = len(y_data)\n",
    "    razao = distribuicao.max() / distribuicao.min() if len(distribuicao) > 1 else 1.0\n",
    "    \n",
    "    print(f\"\\n{nome.upper()}:\")\n",
    "    print(f\"   Total de amostras: {total:,}\")\n",
    "    for classe, qtd in distribuicao.items():\n",
    "        perc = (qtd/total)*100\n",
    "        print(f\"   Classe {classe}: {qtd:,} ({perc:.1f}%)\")\n",
    "    print(f\"   Razão de balanceamento: {razao:.2f}:1\")\n",
    "\n",
    "if len(datasets_balanceados) > 1:\n",
    "    fig, axes = plt.subplots(2, len(datasets_balanceados), figsize=(4*len(datasets_balanceados), 8))\n",
    "    if len(datasets_balanceados) == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for i, (nome, (X_data, y_data)) in enumerate(datasets_balanceados.items()):\n",
    "        dist = pd.Series(y_data).value_counts().sort_index()\n",
    "        axes[0, i].bar(range(len(dist)), dist.values, color=['skyblue', 'lightcoral'])\n",
    "        axes[0, i].set_title(f'{nome}\\n({len(y_data):,} amostras)', fontsize=12, fontweight='bold')\n",
    "        axes[0, i].set_xlabel('Classe')\n",
    "        axes[0, i].set_ylabel('Quantidade')\n",
    "        \n",
    "        for j, v in enumerate(dist.values):\n",
    "            axes[0, i].text(j, v + max(dist.values)*0.01, f'{v:,}', ha='center', fontweight='bold')\n",
    "        \n",
    "        axes[1, i].pie(dist.values, labels=[f'Classe {j}' for j in range(len(dist))], \n",
    "                      autopct='%1.1f%%', startangle=90)\n",
    "        axes[1, i].set_title(f'Proporção - {nome}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "dataset_escolhido = 'SMOTE' if 'SMOTE' in datasets_balanceados else 'Original'\n",
    "X_balanceado, y_balanceado = datasets_balanceados[dataset_escolhido]\n",
    "\n",
    "print(f\"\\nDataset escolhido para modelagem: {dataset_escolhido}\")\n",
    "print(f\"Shape final: X={X_balanceado.shape}, y={y_balanceado.shape}\")\n",
    "print(\"Técnicas de balanceamento aplicadas com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab9aeb",
   "metadata": {},
   "source": [
    "## 3. Análise de Correlação e Seleção de Variáveis\n",
    "\n",
    "### Objetivos:\n",
    "1. **Visualizar matriz de correlação** entre todas as variáveis\n",
    "2. **Identificar correlações com o target** (variáveis mais relevantes)\n",
    "3. **Investigar relações específicas** entre tempo de contrato e evasão\n",
    "4. **Analisar impacto dos gastos** na decisão de cancelamento\n",
    "5. **Selecionar features mais relevantes** para modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef56669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa 1: Divisão Estratificada dos Dados\n",
    "\n",
    "print(\"Divisão dos dados em treino e teste\")\n",
    "\n",
    "try:\n",
    "    if 'features_recomendadas' in locals() and 'X_final' in locals() and 'y_encoded' in locals():\n",
    "        X_modelagem = X_final[features_recomendadas[:15]]\n",
    "        y_modelagem = y_encoded\n",
    "        print(\"Usando features selecionadas da análise de correlação\")\n",
    "        print(f\"Features utilizadas ({len(X_modelagem.columns)}): {list(X_modelagem.columns)}\")\n",
    "    else:\n",
    "        X_modelagem = X_final\n",
    "        y_modelagem = y_encoded\n",
    "        print(\"Usando todas as features processadas disponíveis\")\n",
    "        \n",
    "except NameError:\n",
    "    print(\"Executando pré-processamento básico...\")\n",
    "    \n",
    "    if 'Churn' in dados.columns:\n",
    "        X_modelagem = dados.drop('Churn', axis=1)\n",
    "        y_modelagem = dados['Churn']\n",
    "        coluna_target = 'Churn'\n",
    "    else:\n",
    "        X_modelagem = dados.iloc[:, :-1]\n",
    "        y_modelagem = dados.iloc[:, -1]\n",
    "        coluna_target = dados.columns[-1]\n",
    "    \n",
    "    for col in X_modelagem.select_dtypes(include=['object']).columns:\n",
    "        if col != 'customerID':\n",
    "            le = LabelEncoder()\n",
    "            X_modelagem[col] = le.fit_transform(X_modelagem[col].astype(str))\n",
    "    \n",
    "    id_columns = [col for col in X_modelagem.columns if 'id' in col.lower()]\n",
    "    if id_columns:\n",
    "        X_modelagem = X_modelagem.drop(columns=id_columns)\n",
    "        print(f\"Removidas colunas ID: {id_columns}\")\n",
    "    \n",
    "    print(\"Pré-processamento básico aplicado\")\n",
    "\n",
    "print(f\"\\nDimensões dos dados:\")\n",
    "print(f\"   • Features (X): {X_modelagem.shape}\")\n",
    "print(f\"   • Target (y): {y_modelagem.shape}\")\n",
    "print(f\"   • Total de amostras: {len(y_modelagem):,}\")\n",
    "\n",
    "distribuicao_original = pd.Series(y_modelagem).value_counts()\n",
    "print(f\"\\nDistribuição original das classes:\")\n",
    "for classe, qtd in distribuicao_original.items():\n",
    "    percentual = (qtd / len(y_modelagem)) * 100\n",
    "    print(f\"   • Classe {classe}: {qtd:,} amostras ({percentual:.1f}%)\")\n",
    "\n",
    "divisoes = [\n",
    "    {'test_size': 0.2, 'nome': '80/20', 'recomendacao': 'PADRÃO'},\n",
    "    {'test_size': 0.3, 'nome': '70/30', 'recomendacao': 'ALTERNATIVA'}\n",
    "]\n",
    "\n",
    "print(f\"\\nTestando diferentes divisões:\")\n",
    "\n",
    "datasets_divididos = {}\n",
    "\n",
    "for config in divisoes:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_modelagem, y_modelagem,\n",
    "        test_size=config['test_size'],\n",
    "        random_state=42,\n",
    "        stratify=y_modelagem\n",
    "    )\n",
    "    \n",
    "    datasets_divididos[config['nome']] = {\n",
    "        'X_train': X_train, 'X_test': X_test,\n",
    "        'y_train': y_train, 'y_test': y_test,\n",
    "        'config': config\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nDivisão {config['nome']} ({config['recomendacao']}):\")\n",
    "    print(f\"   • Treino: {X_train.shape[0]:,} amostras ({(1-config['test_size'])*100:.0f}%)\")\n",
    "    print(f\"   • Teste:  {X_test.shape[0]:,} amostras ({config['test_size']*100:.0f}%)\")\n",
    "    \n",
    "    dist_treino = pd.Series(y_train).value_counts(normalize=True)\n",
    "    dist_teste = pd.Series(y_test).value_counts(normalize=True)\n",
    "    \n",
    "    print(\"   Distribuição Treino vs Teste:\")\n",
    "    for classe in dist_treino.index:\n",
    "        diff = abs(dist_treino[classe] - dist_teste[classe]) * 100\n",
    "        status = \"OK\" if diff < 2 else \"Alerta\"\n",
    "        print(f\"      {status} Classe {classe}: Treino {dist_treino[classe]:.1%} | Teste {dist_teste[classe]:.1%} | Diff: {diff:.1f}pp\")\n",
    "\n",
    "divisao_escolhida = '80/20'\n",
    "dados_finais = datasets_divididos[divisao_escolhida]\n",
    "\n",
    "X_train = dados_finais['X_train']\n",
    "X_test = dados_finais['X_test']\n",
    "y_train = dados_finais['y_train'] \n",
    "y_test = dados_finais['y_test']\n",
    "\n",
    "print(f\"\\nDivisão escolhida: {divisao_escolhida}\")\n",
    "print(f\"Dados preparados para modelagem:\")\n",
    "print(f\"   • X_train: {X_train.shape}\")\n",
    "print(f\"   • X_test: {X_test.shape}\")\n",
    "print(f\"   • y_train: {y_train.shape}\")\n",
    "print(f\"   • y_test: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nDivisão estratificada dos dados concluída\")\n",
    "print(\"Dados prontos para treinamento dos modelos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ee111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa 2: Preparação Específica por Tipo de Modelo\n",
    "\n",
    "print(\"Preparação dos dados por categoria de modelo\")\n",
    "\n",
    "categorias_modelos = {\n",
    "    'Sensíveis à Escala': {\n",
    "        'modelos': ['Logistic Regression', 'KNN'],\n",
    "        'precisa_normalizacao': True,\n",
    "        'justificativa': [\n",
    "            'Regressão Logística usa gradiente descendente (sensível a diferentes escalas)',\n",
    "            'KNN calcula distâncias euclidianas (features com escalas maiores dominam)',\n",
    "            'Convergência mais rápida e estável com dados normalizados',\n",
    "            'Coeficientes mais interpretáveis na Regressão Logística'\n",
    "        ]\n",
    "    },\n",
    "    'Baseados em Árvore': {\n",
    "        'modelos': ['Decision Tree', 'Random Forest'],\n",
    "        'precisa_normalizacao': False,\n",
    "        'justificativa': [\n",
    "            'Árvores fazem divisões baseadas em valores absolutos (thresholds)',\n",
    "            'Algoritmo é invariante a transformações monótonas',\n",
    "            'Cada feature é avaliada independentemente',\n",
    "            'Normalização não afeta a estrutura das decisões'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Justificativas técnicas por categoria:\")\n",
    "\n",
    "for categoria, info in categorias_modelos.items():\n",
    "    necessidade = \"REQUER\" if info['precisa_normalizacao'] else \"NÃO REQUER\"\n",
    "    \n",
    "    print(f\"\\n{categoria.upper()} - {necessidade} NORMALIZAÇÃO:\")\n",
    "    print(f\"   • Modelos: {', '.join(info['modelos'])}\")\n",
    "    print(\"   • Justificativas técnicas:\")\n",
    "    for just in info['justificativa']:\n",
    "        print(f\"     ○ {just}\")\n",
    "\n",
    "datasets_por_categoria = {}\n",
    "\n",
    "print(f\"\\nPreparando datasets específicos:\")\n",
    "\n",
    "print(\"\\nCATEGORIA: Modelos Baseados em Árvore\")\n",
    "print(\"   Estratégia: Dados originais (sem normalização)\")\n",
    "\n",
    "datasets_por_categoria['Baseados em Árvore'] = {\n",
    "    'X_train': X_train.copy(),\n",
    "    'X_test': X_test.copy(),\n",
    "    'y_train': y_train.copy(),\n",
    "    'y_test': y_test.copy(),\n",
    "    'scaler': None,\n",
    "    'normalizacao_aplicada': False\n",
    "}\n",
    "\n",
    "print(\"   Dataset preparado (dados mantidos na escala original)\")\n",
    "\n",
    "print(\"\\nCATEGORIA: Modelos Sensíveis à Escala\")\n",
    "print(\"   Estratégia: StandardScaler (média=0, std=1)\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "datasets_por_categoria['Sensíveis à Escala'] = {\n",
    "    'X_train': X_train_scaled,\n",
    "    'X_test': X_test_scaled,\n",
    "    'y_train': y_train.copy(),\n",
    "    'y_test': y_test.copy(),\n",
    "    'scaler': scaler,\n",
    "    'normalizacao_aplicada': True\n",
    "}\n",
    "\n",
    "print(\"   StandardScaler aplicado\")\n",
    "\n",
    "print(f\"\\nComparando escalas antes e depois:\")\n",
    "\n",
    "# Amostra de features para comparação\n",
    "features_amostra = X_train.columns[:5]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Antes da normalização\n",
    "axes[0,0].boxplot([X_train[col] for col in features_amostra], labels=features_amostra)\n",
    "axes[0,0].set_title('ANTES - Escalas Originais')\n",
    "axes[0,0].set_ylabel('Valor')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "axes[0,0].grid(alpha=0.3)\n",
    "\n",
    "# Depois da normalização\n",
    "axes[0,1].boxplot([X_train_scaled[col] for col in features_amostra], labels=features_amostra)\n",
    "axes[0,1].set_title('DEPOIS - Escalas Padronizadas')\n",
    "axes[0,1].set_ylabel('Valor (Z-score)')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].grid(alpha=0.3)\n",
    "\n",
    "# Histograma antes\n",
    "for i, col in enumerate(features_amostra[:2]):\n",
    "    axes[1,0].hist(X_train[col], alpha=0.7, bins=20, label=col)\n",
    "axes[1,0].set_title('ANTES - Distribuições Originais')\n",
    "axes[1,0].set_xlabel('Valor')\n",
    "axes[1,0].set_ylabel('Frequência')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(alpha=0.3)\n",
    "\n",
    "# Histograma depois\n",
    "for i, col in enumerate(features_amostra[:2]):\n",
    "    axes[1,1].hist(X_train_scaled[col], alpha=0.7, bins=20, label=col)\n",
    "axes[1,1].set_title('DEPOIS - Escalas Padronizadas')\n",
    "axes[1,1].set_ylabel('Valor (Z-score)')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "axes[1,1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nResumo dos datasets preparados:\")\n",
    "\n",
    "for categoria, dados in datasets_por_categoria.items():\n",
    "    print(f\"\\n{categoria.upper()}:\")\n",
    "    print(f\"   • Normalização: {'SIM' if dados['normalizacao_aplicada'] else 'NÃO'}\")\n",
    "    print(f\"   • X_train shape: {dados['X_train'].shape}\")\n",
    "    print(f\"   • X_test shape: {dados['X_test'].shape}\")\n",
    "    print(f\"   • Modelos compatíveis: {', '.join(categorias_modelos[categoria]['modelos'])}\")\n",
    "    if dados['scaler']:\n",
    "        print(f\"   • Scaler: {type(dados['scaler']).__name__}\")\n",
    "\n",
    "print(f\"\\nPreparação específica por tipo de modelo concluída\")\n",
    "print(\"Datasets otimizados prontos para treinamento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a537ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa 3: Treinamento dos Modelos Selecionados\n",
    "\n",
    "print(\"Treinamento dos modelos preditivos\")\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "modelos_config = {\n",
    "    'Regressão Logística': {\n",
    "        'modelo': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'categoria': 'Sensíveis à Escala',\n",
    "        'dataset_key': 'Sensíveis à Escala',\n",
    "        'justificativa': [\n",
    "            'Modelo linear baseado em probabilidades (função sigmoide)',\n",
    "            'Coeficientes interpretáveis para análise de impacto',\n",
    "            'Computacionalmente eficiente para datasets grandes',\n",
    "            'Baseline robusto para problemas de classificação binária'\n",
    "        ],\n",
    "        'vantagens': ['Interpretabilidade alta', 'Rápido treinamento', 'Probabilidades calibradas'],\n",
    "        'quando_usar': 'Relações lineares entre features e target'\n",
    "    },\n",
    "    \n",
    "    'K-Nearest Neighbors': {\n",
    "        'modelo': KNeighborsClassifier(n_neighbors=5, metric='euclidean'),\n",
    "        'categoria': 'Sensíveis à Escala',\n",
    "        'dataset_key': 'Sensíveis à Escala',\n",
    "        'justificativa': [\n",
    "            'Algoritmo não-paramétrico baseado em vizinhança',\n",
    "            'Captura padrões locais complexos nos dados',\n",
    "            'Flexível para fronteiras de decisão não-lineares',\n",
    "            'Não assume distribuição específica dos dados'\n",
    "        ],\n",
    "        'vantagens': ['Sem suposições sobre dados', 'Fronteiras complexas', 'Simples conceitualmente'],\n",
    "        'quando_usar': 'Padrões locais complexos, dados com clusters naturais'\n",
    "    },\n",
    "    \n",
    "    'Árvore de Decisão': {\n",
    "        'modelo': DecisionTreeClassifier(random_state=42, max_depth=10, min_samples_split=20),\n",
    "        'categoria': 'Baseados em Árvore',\n",
    "        'dataset_key': 'Baseados em Árvore',\n",
    "        'justificativa': [\n",
    "            'Modelo de regras interpretáveis (if-then-else)',\n",
    "            'Captura interações não-lineares entre features',\n",
    "            'Seleção automática de features mais importantes',\n",
    "            'Não afetado por outliers e diferentes escalas'\n",
    "        ],\n",
    "        'vantagens': ['Máxima interpretabilidade', 'Sem pré-processamento', 'Captura interações'],\n",
    "        'quando_usar': 'Necessidade de explicabilidade total das decisões'\n",
    "    },\n",
    "    \n",
    "    'Random Forest': {\n",
    "        'modelo': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),\n",
    "        'categoria': 'Baseados em Árvore',\n",
    "        'dataset_key': 'Baseados em Árvore',\n",
    "        'justificativa': [\n",
    "            'Ensemble de árvores reduz overfitting individual',\n",
    "            'Robustez através de bootstrap aggregating (bagging)',\n",
    "            'Feature importance automática e confiável',\n",
    "            'Excelente performance geral na maioria dos problemas'\n",
    "        ],\n",
    "        'vantagens': ['Alta precisão', 'Robusto a overfitting', 'Feature importance'],\n",
    "        'quando_usar': 'Quando performance é prioridade sobre interpretabilidade'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Justificativas detalhadas para cada modelo:\")\n",
    "\n",
    "for nome_modelo, config in modelos_config.items():\n",
    "    necessidade = \"REQUER\" if config['categoria'] == 'Sensíveis à Escala' else \"NÃO REQUER\"\n",
    "    \n",
    "    print(f\"\\n{nome_modelo.upper()} ({config['categoria']}) - {necessidade} NORMALIZAÇÃO:\")\n",
    "    print(\"   Justificativas técnicas:\")\n",
    "    for just in config['justificativa']:\n",
    "        print(f\"      • {just}\")\n",
    "    print(f\"   Vantagens: {', '.join(config['vantagens'])}\")\n",
    "    print(f\"   Quando usar: {config['quando_usar']}\")\n",
    "\n",
    "resultados_modelos = {}\n",
    "\n",
    "print(f\"\\nIniciando treinamento dos modelos...\")\n",
    "\n",
    "for nome_modelo, config in modelos_config.items():\n",
    "    print(f\"\\nTreinando: {nome_modelo}\")\n",
    "    \n",
    "    dataset = datasets_por_categoria[config['dataset_key']]\n",
    "    X_train_modelo = dataset['X_train'].copy()\n",
    "    X_test_modelo = dataset['X_test'].copy()\n",
    "    y_train_modelo = dataset['y_train']\n",
    "    y_test_modelo = dataset['y_test']\n",
    "    \n",
    "    # Verificar e tratar valores NaN\n",
    "    nan_count_train = X_train_modelo.isnull().sum().sum()\n",
    "    nan_count_test = X_test_modelo.isnull().sum().sum()\n",
    "    \n",
    "    if nan_count_train > 0 or nan_count_test > 0:\n",
    "        print(f\"   Valores NaN encontrados: Train={nan_count_train}, Test={nan_count_test}\")\n",
    "        print(\"   Aplicando tratamento de valores ausentes (preenchimento com mediana)\")\n",
    "        \n",
    "        # Preencher NaNs com a mediana para colunas numéricas\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        \n",
    "        X_train_modelo = pd.DataFrame(\n",
    "            imputer.fit_transform(X_train_modelo), \n",
    "            columns=X_train_modelo.columns, \n",
    "            index=X_train_modelo.index\n",
    "        )\n",
    "        \n",
    "        X_test_modelo = pd.DataFrame(\n",
    "            imputer.transform(X_test_modelo), \n",
    "            columns=X_test_modelo.columns, \n",
    "            index=X_test_modelo.index\n",
    "        )\n",
    "        \n",
    "        print(\"   Valores NaN tratados com sucesso\")\n",
    "    \n",
    "    normalizacao_status = \"COM\" if dataset['normalizacao_aplicada'] else \"SEM\"\n",
    "    print(f\"   Dataset: {normalizacao_status} normalização\")\n",
    "    \n",
    "    inicio = time.time()\n",
    "    modelo = config['modelo']\n",
    "    modelo.fit(X_train_modelo, y_train_modelo)\n",
    "    fim = time.time()\n",
    "    tempo_treinamento = fim - inicio\n",
    "    \n",
    "    print(f\"   Tempo de treinamento: {tempo_treinamento:.3f} segundos\")\n",
    "    \n",
    "    inicio_pred = time.time()\n",
    "    y_pred = modelo.predict(X_test_modelo)\n",
    "    y_pred_proba = modelo.predict_proba(X_test_modelo)[:, 1]\n",
    "    fim_pred = time.time()\n",
    "    tempo_predicao = fim_pred - inicio_pred\n",
    "    \n",
    "    print(f\"   Tempo de predição: {tempo_predicao:.3f} segundos\")\n",
    "    \n",
    "    accuracy = accuracy_score(y_test_modelo, y_pred)\n",
    "    precision = precision_score(y_test_modelo, y_pred)\n",
    "    recall = recall_score(y_test_modelo, y_pred)\n",
    "    f1 = f1_score(y_test_modelo, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test_modelo, y_pred_proba)\n",
    "    \n",
    "    print(\"   Executando validação cruzada...\")\n",
    "    # Aplicar mesmo tratamento de NaN para validação cruzada\n",
    "    X_train_cv = X_train_modelo.copy()\n",
    "    if X_train_cv.isnull().sum().sum() > 0:\n",
    "        X_train_cv = pd.DataFrame(\n",
    "            SimpleImputer(strategy='median').fit_transform(X_train_cv),\n",
    "            columns=X_train_cv.columns,\n",
    "            index=X_train_cv.index\n",
    "        )\n",
    "    \n",
    "    cv_scores = cross_val_score(config['modelo'], X_train_cv, y_train_modelo, \n",
    "                               cv=5, scoring='f1', n_jobs=-1)\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    resultados_modelos[nome_modelo] = {\n",
    "        'modelo': modelo,\n",
    "        'categoria': config['categoria'],\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'cv_f1_mean': cv_mean,\n",
    "        'cv_f1_std': cv_std,\n",
    "        'tempo_treinamento': tempo_treinamento,\n",
    "        'tempo_predicao': tempo_predicao,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'y_test': y_test_modelo,\n",
    "        'dataset_usado': config['dataset_key'],\n",
    "        'normalizacao': dataset['normalizacao_aplicada']\n",
    "    }\n",
    "    \n",
    "    print(f\"   Métricas no teste:\")\n",
    "    print(f\"      • Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"      • F1-Score: {f1:.4f}\")\n",
    "    print(f\"      • ROC-AUC:  {roc_auc:.4f}\")\n",
    "    print(f\"   Validação Cruzada F1: {cv_mean:.4f} (±{cv_std:.4f})\")\n",
    "    \n",
    "    print(f\"   {nome_modelo} treinado com sucesso\")\n",
    "\n",
    "print(f\"\\nTodos os {len(modelos_config)} modelos treinados com sucesso\")\n",
    "print(\"Prontos para avaliação comparativa detalhada\")\n",
    "\n",
    "print(f\"\\nVariáveis criadas:\")\n",
    "print(f\"   • resultados_modelos: {len(resultados_modelos)} modelos treinados\")\n",
    "print(f\"   • modelos_config: Configurações e justificativas\")\n",
    "print(f\"   • datasets_por_categoria: Datasets específicos por tipo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481c6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa 4: Avaliação Comparativa dos Modelos\n",
    "\n",
    "print(\"Comparação detalhada dos modelos treinados\")\n",
    "\n",
    "df_comparacao = pd.DataFrame({\n",
    "    nome: {\n",
    "        'Accuracy': resultado['accuracy'],\n",
    "        'Precision': resultado['precision'],\n",
    "        'Recall': resultado['recall'],\n",
    "        'F1-Score': resultado['f1_score'],\n",
    "        'ROC-AUC': resultado['roc_auc'],\n",
    "        'CV F1 Mean': resultado['cv_f1_mean'],\n",
    "        'CV F1 Std': resultado['cv_f1_std'],\n",
    "        'Tempo Treino (s)': resultado['tempo_treinamento'],\n",
    "        'Tempo Pred (s)': resultado['tempo_predicao'],\n",
    "        'Normalização': 'Sim' if resultado['normalizacao'] else 'Não'\n",
    "    }\n",
    "    for nome, resultado in resultados_modelos.items()\n",
    "}).T\n",
    "\n",
    "print(\"Tabela comparativa completa:\")\n",
    "print(df_comparacao.round(4))\n",
    "\n",
    "print(f\"\\nMelhores modelos por métrica:\")\n",
    "\n",
    "metricas_principais = ['Accuracy', 'F1-Score', 'ROC-AUC', 'CV F1 Mean']\n",
    "for metrica in metricas_principais:\n",
    "    melhor_modelo = df_comparacao[metrica].idxmax()\n",
    "    melhor_valor = df_comparacao[metrica].max()\n",
    "    print(f\"   • {metrica:12s}: {melhor_modelo} ({melhor_valor:.4f})\")\n",
    "\n",
    "print(f\"\\nRanking geral (média das métricas normalizadas):\")\n",
    "\n",
    "metricas_para_ranking = ['Accuracy', 'F1-Score', 'ROC-AUC', 'CV F1 Mean']\n",
    "df_normalizado = df_comparacao[metricas_para_ranking].copy()\n",
    "\n",
    "for metrica in metricas_para_ranking:\n",
    "    min_val = df_normalizado[metrica].min()\n",
    "    max_val = df_normalizado[metrica].max()\n",
    "    df_normalizado[metrica] = (df_normalizado[metrica] - min_val) / (max_val - min_val)\n",
    "\n",
    "df_normalizado['Score_Geral'] = df_normalizado.mean(axis=1)\n",
    "ranking_geral = df_normalizado.sort_values('Score_Geral', ascending=False)\n",
    "\n",
    "for i, (modelo, row) in enumerate(ranking_geral.iterrows(), 1):\n",
    "    posicao = f\"{i}°\" \n",
    "    print(f\"{posicao} {modelo:20s} | Score: {row['Score_Geral']:.4f}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "metricas_viz = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "df_viz = df_comparacao[metricas_viz]\n",
    "\n",
    "ax = axes[0]\n",
    "df_viz.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('Comparação de Métricas por Modelo', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Modelos')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "ax = axes[1]\n",
    "tempos_treino = df_comparacao['Tempo Treino (s)'].sort_values()\n",
    "cores_tempo = ['green' if x < 0.1 else 'orange' if x < 1 else 'red' for x in tempos_treino]\n",
    "tempos_treino.plot(kind='bar', ax=ax, color=cores_tempo)\n",
    "ax.set_title('Tempo de Treinamento por Modelo', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Segundos')\n",
    "ax.set_xlabel('Modelos')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "ax = axes[2]\n",
    "for modelo, resultado in resultados_modelos.items():\n",
    "    cor = 'blue' if 'Sensíveis à Escala' in resultado['categoria'] else 'red'\n",
    "    ax.scatter(resultado['accuracy'], resultado['f1_score'], \n",
    "              s=100, c=cor, alpha=0.7, label=modelo)\n",
    "ax.set_xlabel('Accuracy')\n",
    "ax.set_ylabel('F1-Score')\n",
    "ax.set_title('Accuracy vs F1-Score', fontsize=12, fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "ax = axes[3]\n",
    "modelos_nomes = list(resultados_modelos.keys())\n",
    "cv_means = [resultados_modelos[m]['cv_f1_mean'] for m in modelos_nomes]\n",
    "cv_stds = [resultados_modelos[m]['cv_f1_std'] for m in modelos_nomes]\n",
    "\n",
    "bars = ax.bar(range(len(modelos_nomes)), cv_means, yerr=cv_stds, \n",
    "              capsize=5, alpha=0.7, color=['blue', 'blue', 'red', 'red'])\n",
    "ax.set_xticks(range(len(modelos_nomes)))\n",
    "ax.set_xticklabels([nome[:15] + '...' if len(nome) > 15 else nome for nome in modelos_nomes], rotation=45)\n",
    "ax.set_ylabel('F1-Score')\n",
    "ax.set_title('Validação Cruzada (5-fold) - F1-Score', fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "melhor_modelo_nome = ranking_geral.index[0]\n",
    "melhor_resultado = resultados_modelos[melhor_modelo_nome]\n",
    "\n",
    "cm = confusion_matrix(melhor_resultado['y_test'], melhor_resultado['y_pred'])\n",
    "ax = axes[4]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Não Cancelou', 'Cancelou'],\n",
    "            yticklabels=['Não Cancelou', 'Cancelou'])\n",
    "ax.set_title(f'Matriz de Confusão\\n{melhor_modelo_nome}', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Predição')\n",
    "ax.set_ylabel('Real')\n",
    "\n",
    "ax = axes[5]\n",
    "colors = ['blue', 'green', 'red', 'orange']\n",
    "for i, (nome_modelo, resultado) in enumerate(resultados_modelos.items()):\n",
    "    fpr, tpr, _ = roc_curve(resultado['y_test'], resultado['y_pred_proba'])\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, color=colors[i], lw=2, \n",
    "            label=f'{nome_modelo[:15]}... (AUC={auc_score:.3f})')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random (AUC=0.5)')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('Taxa de Falsos Positivos')\n",
    "ax.set_ylabel('Taxa de Verdadeiros Positivos')\n",
    "ax.set_title('Curvas ROC - Comparação', fontsize=12, fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAnálise por categoria de modelo:\")\n",
    "\n",
    "categorias_performance = {}\n",
    "for nome_modelo, resultado in resultados_modelos.items():\n",
    "    categoria = resultado['categoria']\n",
    "    if categoria not in categorias_performance:\n",
    "        categorias_performance[categoria] = []\n",
    "    categorias_performance[categoria].append({\n",
    "        'nome': nome_modelo,\n",
    "        'f1': resultado['f1_score'],\n",
    "        'auc': resultado['roc_auc'],\n",
    "        'tempo': resultado['tempo_treinamento']\n",
    "    })\n",
    "\n",
    "for categoria, modelos in categorias_performance.items():\n",
    "    print(f\"\\n{categoria.upper()}:\")\n",
    "    f1_medio = np.mean([m['f1'] for m in modelos])\n",
    "    auc_medio = np.mean([m['auc'] for m in modelos])\n",
    "    tempo_medio = np.mean([m['tempo'] for m in modelos])\n",
    "    \n",
    "    print(f\"   Performance média: F1={f1_medio:.4f}, AUC={auc_medio:.4f}\")\n",
    "    print(f\"   Tempo médio de treino: {tempo_medio:.4f}s\")\n",
    "    print(f\"   Melhor modelo: {max(modelos, key=lambda x: x['f1'])['nome']}\")\n",
    "\n",
    "print(f\"\\nRecomendação final:\")\n",
    "\n",
    "melhor_geral = ranking_geral.index[0]\n",
    "melhor_resultado_final = resultados_modelos[melhor_geral]\n",
    "\n",
    "print(f\"MODELO RECOMENDADO: {melhor_geral}\")\n",
    "print(f\"   F1-Score: {melhor_resultado_final['f1_score']:.4f}\")\n",
    "print(f\"   ROC-AUC: {melhor_resultado_final['roc_auc']:.4f}\")\n",
    "print(f\"   Tempo treino: {melhor_resultado_final['tempo_treinamento']:.4f}s\")\n",
    "print(f\"   Normalização: {'Sim' if melhor_resultado_final['normalizacao'] else 'Não'}\")\n",
    "\n",
    "print(f\"\\nAvaliação comparativa concluída\")\n",
    "print(\"Modelo recomendado selecionado para implementação\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d386ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa 5: Síntese da Modelagem Preditiva\n",
    "\n",
    "print(\"Síntese completa da modelagem preditiva\")\n",
    "\n",
    "print(\"Estratégia de modelagem implementada:\")\n",
    "\n",
    "print(\"1. DIVISÃO DOS DADOS:\")\n",
    "print(f\"     • Estratégia: {divisao_escolhida} (treino/teste)\")\n",
    "print(\"     • Método: Divisão estratificada para manter proporção das classes\")\n",
    "print(\"     • Validação: Verificação da distribuição em treino e teste\")\n",
    "\n",
    "print(\"\\n2. PREPARAÇÃO ESPECÍFICA POR MODELO:\")\n",
    "print(\"     • Modelos sensíveis à escala: Normalização com StandardScaler\")\n",
    "print(\"     • Modelos baseados em árvore: Dados na escala original\")\n",
    "print(\"     • Justificativa: Otimização específica para cada tipo de algoritmo\")\n",
    "\n",
    "print(\"\\n3. SELEÇÃO E TREINAMENTO DOS MODELOS:\")\n",
    "algoritmos_implementados = list(modelos_config.keys())\n",
    "print(f\"     • Total de algoritmos: {len(algoritmos_implementados)}\")\n",
    "for algo in algoritmos_implementados:\n",
    "    print(f\"       - {algo}\")\n",
    "\n",
    "print(\"\\n4. VALIDAÇÃO E AVALIAÇÃO:\")\n",
    "print(\"     • Métricas: Accuracy, Precision, Recall, F1-Score, ROC-AUC\")\n",
    "print(\"     • Validação cruzada: 5-fold stratified\")\n",
    "print(\"     • Comparação: Ranking baseado em métricas normalizadas\")\n",
    "\n",
    "melhor_modelo_final = ranking_geral.index[0]\n",
    "resultado_melhor = resultados_modelos[melhor_modelo_final]\n",
    "\n",
    "print(f\"\\n5. RESULTADO FINAL:\")\n",
    "print(f\"     • Modelo vencedor: {melhor_modelo_final}\")\n",
    "print(f\"     • F1-Score: {resultado_melhor['f1_score']:.4f}\")\n",
    "print(f\"     • ROC-AUC: {resultado_melhor['roc_auc']:.4f}\")\n",
    "print(f\"     • Accuracy: {resultado_melhor['accuracy']:.4f}\")\n",
    "\n",
    "estrategia_implementacao = {\n",
    "    'modelo_final': melhor_modelo_final,\n",
    "    'metricas_finais': {\n",
    "        'f1_score': resultado_melhor['f1_score'],\n",
    "        'roc_auc': resultado_melhor['roc_auc'],\n",
    "        'accuracy': resultado_melhor['accuracy'],\n",
    "        'precision': resultado_melhor['precision'],\n",
    "        'recall': resultado_melhor['recall']\n",
    "    },\n",
    "    'preprocessing_necessario': 'StandardScaler' if resultado_melhor['normalizacao'] else 'Sem normalização',\n",
    "    'tempo_treinamento': resultado_melhor['tempo_treinamento'],\n",
    "    'validacao_cruzada': {\n",
    "        'cv_f1_mean': resultado_melhor['cv_f1_mean'],\n",
    "        'cv_f1_std': resultado_melhor['cv_f1_std']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\n6. INFORMAÇÕES PARA IMPLEMENTAÇÃO:\")\n",
    "print(f\"     • Pré-processamento: {estrategia_implementacao['preprocessing_necessario']}\")\n",
    "print(f\"     • Tempo de treinamento: {estrategia_implementacao['tempo_treinamento']:.4f}s\")\n",
    "print(f\"     • Validação cruzada F1: {estrategia_implementacao['validacao_cruzada']['cv_f1_mean']:.4f} ± {estrategia_implementacao['validacao_cruzada']['cv_f1_std']:.4f}\")\n",
    "\n",
    "print(f\"\\nSíntese da modelagem preditiva concluída\")\n",
    "print(f\"Variável 'estrategia_implementacao' criada com todos os detalhes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55123c5f",
   "metadata": {},
   "source": [
    "## 4. Modelagem Preditiva\n",
    "\n",
    "### Estratégia de Modelagem:\n",
    "\n",
    "**Modelos Selecionados:**\n",
    "- **Regressão Logística** - Linear, interpretável\n",
    "- **K-Nearest Neighbors** - Não-paramétrico, fronteiras complexas  \n",
    "- **Árvore de Decisão** - Regras interpretáveis\n",
    "- **Random Forest** - Ensemble de árvores, robusto\n",
    "\n",
    "**Justificativas Técnicas:**\n",
    "- **Normalização necessária** para modelos baseados em distância/gradiente\n",
    "- **Sem normalização** para modelos baseados em árvore (decisões por limites)\n",
    "- **Comparação justa** entre diferentes abordagens algorítmicas\n",
    "- **Aproveitamento** das análises de correlação e seleção de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883f8de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de Importância das Variáveis por Modelo\n",
    "\n",
    "print(\"ANÁLISE DE IMPORTÂNCIA DAS VARIÁVEIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Regressão Logística - Análise de Coeficientes\n",
    "if 'Regressão Logística' in resultados_modelos:\n",
    "    print(\"\\n1. REGRESSÃO LOGÍSTICA - Análise de Coeficientes\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    modelo_lr = resultados_modelos['Regressão Logística']['modelo']\n",
    "    \n",
    "    # Obter coeficientes\n",
    "    coeficientes = modelo_lr.coef_[0]\n",
    "    \n",
    "    # Obter nomes das features do dataset normalizado\n",
    "    dataset_lr = datasets_por_categoria['Sensíveis à Escala']\n",
    "    feature_names = dataset_lr['X_train'].columns\n",
    "    \n",
    "    # Criar dataframe com coeficientes\n",
    "    df_coef = pd.DataFrame({\n",
    "        'Variável': feature_names,\n",
    "        'Coeficiente': coeficientes,\n",
    "        'Coef_Absoluto': np.abs(coeficientes),\n",
    "        'Odds_Ratio': np.exp(coeficientes),\n",
    "        'Impacto': ['Aumenta Churn' if c > 0 else 'Reduz Churn' for c in coeficientes]\n",
    "    })\n",
    "    \n",
    "    # Ordenar por importância (valor absoluto)\n",
    "    df_coef = df_coef.sort_values('Coef_Absoluto', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 variáveis mais impactantes:\")\n",
    "    for i, row in df_coef.head(10).iterrows():\n",
    "        print(f\"{row.name + 1:2d}. {row['Variável']:25s} | Coef: {row['Coeficiente']:8.4f} | OR: {row['Odds_Ratio']:6.3f} | {row['Impacto']}\")\n",
    "    \n",
    "    # Interpretação dos odds ratios\n",
    "    print(\"\\nInterpretação dos Odds Ratios:\")\n",
    "    for i, row in df_coef.head(5).iterrows():\n",
    "        if row['Odds_Ratio'] > 1:\n",
    "            aumento = (row['Odds_Ratio'] - 1) * 100\n",
    "            print(f\"- {row['Variável']}: Aumenta chance de churn em {aumento:.1f}%\")\n",
    "        else:\n",
    "            reducao = (1 - row['Odds_Ratio']) * 100\n",
    "            print(f\"- {row['Variável']}: Reduz chance de churn em {reducao:.1f}%\")\n",
    "    \n",
    "    # Visualização\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Gráfico dos coeficientes\n",
    "    plt.subplot(1, 2, 1)\n",
    "    top_10_coef = df_coef.head(10)\n",
    "    colors = ['red' if x < 0 else 'blue' for x in top_10_coef['Coeficiente']]\n",
    "    plt.barh(range(len(top_10_coef)), top_10_coef['Coeficiente'], color=colors, alpha=0.7)\n",
    "    plt.yticks(range(len(top_10_coef)), [name[:20] for name in top_10_coef['Variável']])\n",
    "    plt.xlabel('Coeficiente')\n",
    "    plt.title('Top 10 Coeficientes - Regressão Logística')\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Gráfico dos odds ratios\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(range(len(top_10_coef)), top_10_coef['Odds_Ratio'], \n",
    "             color=['green' if x < 1 else 'red' for x in top_10_coef['Odds_Ratio']], alpha=0.7)\n",
    "    plt.yticks(range(len(top_10_coef)), [name[:20] for name in top_10_coef['Variável']])\n",
    "    plt.xlabel('Odds Ratio')\n",
    "    plt.title('Top 10 Odds Ratios - Regressão Logística')\n",
    "    plt.axvline(x=1, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Armazenar para relatório final\n",
    "    lr_insights = {\n",
    "        'top_5_vars': df_coef.head(5)['Variável'].tolist(),\n",
    "        'strongest_positive': df_coef[df_coef['Coeficiente'] > 0].iloc[0]['Variável'],\n",
    "        'strongest_negative': df_coef[df_coef['Coeficiente'] < 0].iloc[0]['Variável'] if len(df_coef[df_coef['Coeficiente'] < 0]) > 0 else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82004a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Random Forest - Análise de Importância das Variáveis\n",
    "if 'Random Forest' in resultados_modelos:\n",
    "    print(\"\\n2. RANDOM FOREST - Importância das Variáveis\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    modelo_rf = resultados_modelos['Random Forest']['modelo']\n",
    "    \n",
    "    importancias = modelo_rf.feature_importances_\n",
    "    \n",
    "    dataset_rf = datasets_por_categoria['Baseados em Árvore']\n",
    "    feature_names_rf = dataset_rf['X_train'].columns\n",
    "    \n",
    "    df_importancia = pd.DataFrame({\n",
    "        'Variável': feature_names_rf,\n",
    "        'Importância': importancias,\n",
    "        'Importância_Pct': importancias * 100\n",
    "    })\n",
    "    \n",
    "    df_importancia = df_importancia.sort_values('Importância', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 variáveis mais importantes:\")\n",
    "    for i, row in df_importancia.head(10).iterrows():\n",
    "        print(f\"{i+1:2d}. {row['Variável']:25s} | Importância: {row['Importância']:6.4f} ({row['Importância_Pct']:5.1f}%)\")\n",
    "    \n",
    "    # Calcular importância cumulativa\n",
    "    df_importancia['Importância_Cumulativa'] = df_importancia['Importância'].cumsum()\n",
    "    \n",
    "    print(f\"\\nImportância cumulativa:\")\n",
    "    print(f\"Top 5 variáveis: {df_importancia.head(5)['Importância_Cumulativa'].iloc[-1]:.1%}\")\n",
    "    print(f\"Top 10 variáveis: {df_importancia.head(10)['Importância_Cumulativa'].iloc[-1]:.1%}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    top_10_imp = df_importancia.head(10)\n",
    "    plt.barh(range(len(top_10_imp)), top_10_imp['Importância'], color='forestgreen', alpha=0.7)\n",
    "    plt.yticks(range(len(top_10_imp)), [name[:20] for name in top_10_imp['Variável']])\n",
    "    plt.xlabel('Importância')\n",
    "    plt.title('Top 10 Importância - Random Forest')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(df_importancia.head(15)) + 1), \n",
    "             df_importancia.head(15)['Importância_Cumulativa'], 'o-', color='darkgreen')\n",
    "    plt.xlabel('Número de Variáveis')\n",
    "    plt.ylabel('Importância Cumulativa')\n",
    "    plt.title('Importância Cumulativa - Top 15')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='80%')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Armazenar para relatório final\n",
    "    rf_insights = {\n",
    "        'top_5_vars': df_importancia.head(5)['Variável'].tolist(),\n",
    "        'most_important': df_importancia.iloc[0]['Variável'],\n",
    "        'top_5_cumulative': df_importancia.head(5)['Importância_Cumulativa'].iloc[-1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4978a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. K-Nearest Neighbors - Análise por Proximidade\n",
    "if 'K-Nearest Neighbors' in resultados_modelos:\n",
    "    print(\"\\n3. K-NEAREST NEIGHBORS - Análise por Proximidade\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    modelo_knn = resultados_modelos['K-Nearest Neighbors']['modelo']\n",
    "    dataset_knn = datasets_por_categoria['Sensíveis à Escala']\n",
    "    X_test_knn = dataset_knn['X_test']\n",
    "    y_test_knn = dataset_knn['y_test']\n",
    "    \n",
    "    print(\"Análise de influência das variáveis baseada em distâncias:\")\n",
    "\n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "    X_train_knn = dataset_knn['X_train']\n",
    "    y_train_knn = dataset_knn['y_train']\n",
    "    \n",
    "    X_churn = X_train_knn[y_train_knn == 1]\n",
    "    X_no_churn = X_train_knn[y_train_knn == 0]\n",
    "    \n",
    "    # Calcular variâncias das features (maior variância = maior impacto na distância)\n",
    "    feature_variance = X_train_knn.var()\n",
    "    feature_std = X_train_knn.std()\n",
    "    \n",
    "    # análise de relevância baseada na variabilidade\n",
    "    df_knn_relevance = pd.DataFrame({\n",
    "        'Variável': X_train_knn.columns,\n",
    "        'Variância': feature_variance,\n",
    "        'Desvio_Padrão': feature_std,\n",
    "        'Relevância_Normalizada': (feature_std - feature_std.min()) / (feature_std.max() - feature_std.min())\n",
    "    })\n",
    "    \n",
    "    df_knn_relevance = df_knn_relevance.sort_values('Relevância_Normalizada', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 variáveis mais relevantes para cálculo de distância:\")\n",
    "    for i, row in df_knn_relevance.head(10).iterrows():\n",
    "        print(f\"{i+1:2d}. {row['Variável']:25s} | Std: {row['Desvio_Padrão']:6.4f} | Relevância: {row['Relevância_Normalizada']:6.4f}\")\n",
    "    \n",
    "    # relatório final\n",
    "    knn_insights = {\n",
    "        'top_5_vars': df_knn_relevance.head(5)['Variável'].tolist(),\n",
    "        'most_relevant': df_knn_relevance.iloc[0]['Variável']\n",
    "    }\n",
    "\n",
    "# 4. Árvore de Decisão - Análise de Importância\n",
    "if 'Árvore de Decisão' in resultados_modelos:\n",
    "    print(\"\\n4. ÁRVORE DE DECISÃO - Importância das Variáveis\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    modelo_dt = resultados_modelos['Árvore de Decisão']['modelo']\n",
    "    \n",
    "    importancias_dt = modelo_dt.feature_importances_\n",
    "    \n",
    "    #  nomes das features\n",
    "    dataset_dt = datasets_por_categoria['Baseados em Árvore']\n",
    "    feature_names_dt = dataset_dt['X_train'].columns\n",
    "    \n",
    "    # dataframe com importâncias\n",
    "    df_dt_importancia = pd.DataFrame({\n",
    "        'Variável': feature_names_dt,\n",
    "        'Importância': importancias_dt,\n",
    "        'Importância_Pct': importancias_dt * 100\n",
    "    })\n",
    "    \n",
    "    #por importância\n",
    "    df_dt_importancia = df_dt_importancia.sort_values('Importância', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 variáveis mais importantes para divisões:\")\n",
    "    for i, row in df_dt_importancia.head(10).iterrows():\n",
    "        print(f\"{i+1:2d}. {row['Variável']:25s} | Importância: {row['Importância']:6.4f} ({row['Importância_Pct']:5.1f}%)\")\n",
    "\n",
    "    from sklearn import tree\n",
    "    \n",
    "    print(\"\\nPrincipais regras de decisão (primeiros níveis):\")\n",
    "    tree_rules = tree.export_text(modelo_dt, feature_names=list(feature_names_dt), max_depth=3)\n",
    "    print(tree_rules[:500] + \"...\")\n",
    "    \n",
    "    #  relatório final\n",
    "    dt_insights = {\n",
    "        'top_5_vars': df_dt_importancia.head(5)['Variável'].tolist(),\n",
    "        'most_important': df_dt_importancia.iloc[0]['Variável'],\n",
    "        'top_3_importance': df_dt_importancia.head(3)['Importância'].sum()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b1b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidação das Análises de Importância\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONSOLIDAÇÃO DAS ANÁLISES DE IMPORTÂNCIA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Criar dicionário consolidado\n",
    "all_insights = {}\n",
    "if 'lr_insights' in locals():\n",
    "    all_insights['Regressão Logística'] = lr_insights\n",
    "if 'rf_insights' in locals():\n",
    "    all_insights['Random Forest'] = rf_insights  \n",
    "if 'knn_insights' in locals():\n",
    "    all_insights['K-Nearest Neighbors'] = knn_insights\n",
    "if 'dt_insights' in locals():\n",
    "    all_insights['Árvore de Decisão'] = dt_insights\n",
    "\n",
    "print(\"\\nTOP 5 VARIÁVEIS MAIS IMPORTANTES POR MODELO:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for modelo, insights in all_insights.items():\n",
    "    print(f\"\\n{modelo}:\")\n",
    "    for i, var in enumerate(insights['top_5_vars'], 1):\n",
    "        print(f\"  {i}. {var}\")\n",
    "\n",
    "# Análise de consenso entre modelos\n",
    "all_important_vars = []\n",
    "for insights in all_insights.values():\n",
    "    all_important_vars.extend(insights['top_5_vars'])\n",
    "\n",
    "# Contar frequência de cada variável\n",
    "from collections import Counter\n",
    "var_frequency = Counter(all_important_vars)\n",
    "\n",
    "print(\"\\nCONSENSO ENTRE MODELOS - Variáveis mais citadas:\")\n",
    "print(\"-\" * 50)\n",
    "for var, count in var_frequency.most_common(10):\n",
    "    models_citing = [modelo for modelo, insights in all_insights.items() if var in insights['top_5_vars']]\n",
    "    print(f\"{var:30s} | Citada por {count} modelo(s): {', '.join(models_citing)}\")\n",
    "\n",
    "# Visualização comparativa\n",
    "if len(all_insights) > 0:\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Criar matriz de importância por modelo\n",
    "    top_vars_consensus = [var for var, count in var_frequency.most_common(10)]\n",
    "    \n",
    "    # Matriz de presença (1 se está no top 5, 0 caso contrário)\n",
    "    matrix_data = []\n",
    "    model_names = list(all_insights.keys())\n",
    "    \n",
    "    for var in top_vars_consensus:\n",
    "        row = []\n",
    "        for modelo in model_names:\n",
    "            if var in all_insights[modelo]['top_5_vars']:\n",
    "                # Posição na lista (1 = mais importante)\n",
    "                pos = all_insights[modelo]['top_5_vars'].index(var) + 1\n",
    "                row.append(6 - pos)  # Inverter para que 5 = mais importante\n",
    "            else:\n",
    "                row.append(0)\n",
    "        matrix_data.append(row)\n",
    "    \n",
    "    # Heatmap\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(matrix_data, \n",
    "                xticklabels=model_names,\n",
    "                yticklabels=[var[:25] for var in top_vars_consensus],\n",
    "                annot=True, \n",
    "                cmap='YlOrRd', \n",
    "                cbar_kws={'label': 'Importância (5=máxima)'})\n",
    "    plt.title('Consenso de Importância entre Modelos')\n",
    "    plt.xlabel('Modelos')\n",
    "    plt.ylabel('Variáveis')\n",
    "    \n",
    "    # Gráfico de frequência\n",
    "    plt.subplot(1, 2, 2)\n",
    "    vars_plot = [var[:20] for var, _ in var_frequency.most_common(8)]\n",
    "    counts_plot = [count for _, count in var_frequency.most_common(8)]\n",
    "    \n",
    "    plt.barh(range(len(vars_plot)), counts_plot, color='steelblue', alpha=0.7)\n",
    "    plt.yticks(range(len(vars_plot)), vars_plot)\n",
    "    plt.xlabel('Número de Modelos que Citam')\n",
    "    plt.title('Frequência de Citação entre Modelos')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Identificar variáveis de consenso (citadas por pelo menos 50% dos modelos)\n",
    "consensus_threshold = len(all_insights) * 0.5\n",
    "consensus_vars = [var for var, count in var_frequency.items() if count >= consensus_threshold]\n",
    "\n",
    "print(f\"\\nVARIÁVEIS DE CONSENSO (citadas por >= {consensus_threshold:.0f} modelos):\")\n",
    "print(\"-\" * 55)\n",
    "for var in consensus_vars:\n",
    "    count = var_frequency[var]\n",
    "    print(f\"- {var} (citada por {count} modelo(s))\")\n",
    "\n",
    "# Armazenar resultados para relatório final\n",
    "consensus_analysis = {\n",
    "    'top_consensus_vars': consensus_vars[:5] if len(consensus_vars) >= 5 else consensus_vars,\n",
    "    'var_frequency': dict(var_frequency.most_common(10)),\n",
    "    'models_analyzed': list(all_insights.keys())\n",
    "}\n",
    "\n",
    "print(f\"\\nAnálise de importância concluída para {len(all_insights)} modelos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2223648a",
   "metadata": {},
   "source": [
    "## 5. Relatório Final e Estratégias de Retenção\n",
    "\n",
    "### Análise Detalhada dos Fatores de Evasão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e87d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELATÓRIO DETALHADO - FATORES DE EVASÃO E ESTRATÉGIAS DE RETENÇÃO\n",
    "\n",
    "print(\"RELATÓRIO FINAL - ANÁLISE DE EVASÃO TELECOM X\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Recuperar informações dos modelos\n",
    "melhor_modelo = ranking_geral.index[0] if 'ranking_geral' in locals() else \"Não definido\"\n",
    "melhor_performance = resultados_modelos[melhor_modelo] if melhor_modelo in resultados_modelos else {}\n",
    "\n",
    "print(f\"\\nRESUMO EXECUTIVO\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"• Dataset analisado: {len(y_modelagem):,} clientes\" if 'y_modelagem' in locals() else \"• Dataset: Não carregado\")\n",
    "if 'distribuicao_original' in locals():\n",
    "    churn_rate = distribuicao_original[1] / len(y_modelagem) * 100\n",
    "    print(f\"• Taxa de evasão atual: {churn_rate:.1f}%\")\n",
    "print(f\"• Modelos avaliados: {len(resultados_modelos)}\")\n",
    "if melhor_performance:\n",
    "    print(f\"• Melhor modelo: {melhor_modelo}\")\n",
    "    print(f\"• F1-Score alcançado: {melhor_performance['f1_score']:.4f}\")\n",
    "    print(f\"• ROC-AUC alcançado: {melhor_performance['roc_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\n1. PRINCIPAIS FATORES QUE INFLUENCIAM A EVASÃO\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if 'consensus_vars' in locals() and consensus_vars:\n",
    "    print(\"Baseado na análise de consenso entre modelos:\")\n",
    "    for i, var in enumerate(consensus_vars[:5], 1):\n",
    "        models_citing = [modelo for modelo, insights in all_insights.items() if var in insights.get('top_5_vars', [])]\n",
    "        print(f\"  {i}. {var}\")\n",
    "        print(f\"     - Identificada por: {', '.join(models_citing)}\")\n",
    "        \n",
    "        # Adicionar interpretação específica por tipo de variável\n",
    "        if any(term in var.lower() for term in ['tenure', 'tempo']):\n",
    "            print(\"     - Interpretação: Tempo de relacionamento com a empresa\")\n",
    "        elif any(term in var.lower() for term in ['charges', 'total', 'monthly']):\n",
    "            print(\"     - Interpretação: Aspecto financeiro do serviço\")\n",
    "        elif any(term in var.lower() for term in ['contract', 'contrato']):\n",
    "            print(\"     - Interpretação: Tipo de vínculo contratual\")\n",
    "        elif any(term in var.lower() for term in ['service', 'internet', 'phone']):\n",
    "            print(\"     - Interpretação: Tipo de serviço utilizado\")\n",
    "        else:\n",
    "            print(\"     - Interpretação: Fator comportamental/demográfico\")\n",
    "else:\n",
    "    print(\"Execute as análises de importância primeiro.\")\n",
    "\n",
    "print(f\"\\n2. ANÁLISE POR MODELO\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "for modelo, insights in all_insights.items():\n",
    "    print(f\"\\n{modelo}:\")\n",
    "    performance = resultados_modelos[modelo]\n",
    "    print(f\"  Performance: F1={performance['f1_score']:.3f}, AUC={performance['roc_auc']:.3f}\")\n",
    "    print(f\"  Principal fator identificado: {insights.get('most_important', insights.get('top_5_vars', ['N/A'])[0])}\")\n",
    "    \n",
    "    if modelo == 'Regressão Logística':\n",
    "        print(\"  Metodologia: Análise de coeficientes e odds ratios\")\n",
    "        print(\"  Vantagem: Interpretabilidade direta do impacto\")\n",
    "    elif modelo == 'Random Forest':\n",
    "        print(\"  Metodologia: Importância baseada em redução de impureza\")\n",
    "        print(\"  Vantagem: Captura interações complexas entre variáveis\")\n",
    "    elif modelo == 'K-Nearest Neighbors':\n",
    "        print(\"  Metodologia: Relevância baseada em variabilidade para distância\")\n",
    "        print(\"  Vantagem: Identifica padrões locais nos dados\")\n",
    "    elif modelo == 'Árvore de Decisão':\n",
    "        print(\"  Metodologia: Importância baseada em divisões da árvore\")\n",
    "        print(\"  Vantagem: Regras de decisão explícitas\")\n",
    "\n",
    "print(f\"\\n3. ESTRATÉGIAS DE RETENÇÃO BASEADAS NOS RESULTADOS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Estratégias baseadas nos principais fatores identificados\n",
    "strategies = []\n",
    "\n",
    "if 'consensus_vars' in locals():\n",
    "    for var in consensus_vars[:3]:\n",
    "        if any(term in var.lower() for term in ['tenure', 'tempo']):\n",
    "            strategies.append({\n",
    "                'fator': var,\n",
    "                'estrategia': 'Programa de Fidelidade Progressiva',\n",
    "                'acao': 'Criar benefícios crescentes baseados no tempo de relacionamento',\n",
    "                'implementacao': [\n",
    "                    'Descontos progressivos por tempo de permanência',\n",
    "                    'Upgrades gratuitos após períodos específicos',\n",
    "                    'Atendimento prioritário para clientes antigos'\n",
    "                ]\n",
    "            })\n",
    "        elif any(term in var.lower() for term in ['charges', 'total', 'monthly']):\n",
    "            strategies.append({\n",
    "                'fator': var,\n",
    "                'estrategia': 'Otimização de Preços e Pacotes',\n",
    "                'acao': 'Ajustar estrutura de preços para diferentes perfis',\n",
    "                'implementacao': [\n",
    "                    'Análise de sensibilidade ao preço por segmento',\n",
    "                    'Pacotes personalizados baseados no uso',\n",
    "                    'Promoções direcionadas para clientes em risco'\n",
    "                ]\n",
    "            })\n",
    "        elif any(term in var.lower() for term in ['contract', 'contrato']):\n",
    "            strategies.append({\n",
    "                'fator': var,\n",
    "                'estrategia': 'Flexibilização Contratual',\n",
    "                'acao': 'Oferecer opções contratuais mais atrativas',\n",
    "                'implementacao': [\n",
    "                    'Contratos com flexibilidade de mudança',\n",
    "                    'Períodos de teste sem compromisso',\n",
    "                    'Migração facilitada entre planos'\n",
    "                ]\n",
    "            })\n",
    "\n",
    "# Imprimir estratégias\n",
    "for i, strategy in enumerate(strategies, 1):\n",
    "    print(f\"\\nEstratégia {i}: {strategy['estrategia']}\")\n",
    "    print(f\"  Fator relacionado: {strategy['fator']}\")\n",
    "    print(f\"  Objetivo: {strategy['acao']}\")\n",
    "    print(\"  Implementações sugeridas:\")\n",
    "    for impl in strategy['implementacao']:\n",
    "        print(f\"    - {impl}\")\n",
    "\n",
    "print(f\"\\n4. IMPLEMENTAÇÃO DO SISTEMA PREDITIVO\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"Recomendações para operacionalização:\")\n",
    "print(\"\\nA. Scoring de Clientes\")\n",
    "print(\"  - Aplicar modelo preditivo mensalmente\")\n",
    "print(\"  - Classificar clientes por probabilidade de churn\")\n",
    "print(\"  - Criar alertas automáticos para casos de alto risco\")\n",
    "\n",
    "print(\"\\nB. Segmentação para Ações\")\n",
    "if melhor_performance:\n",
    "    threshold_high = 0.7\n",
    "    threshold_medium = 0.4\n",
    "    print(f\"  - Alto risco (p > {threshold_high}): Intervenção imediata\")\n",
    "    print(f\"  - Médio risco ({threshold_medium} < p < {threshold_high}): Acompanhamento próximo\") \n",
    "    print(f\"  - Baixo risco (p < {threshold_medium}): Manutenção regular\")\n",
    "\n",
    "print(\"\\nC. Monitoramento e Ajustes\")\n",
    "print(\"  - Avaliar efetividade das estratégias implementadas\")\n",
    "print(\"  - Retreinar modelo com novos dados trimestralmente\")\n",
    "print(\"  - Acompanhar mudanças nos fatores de risco\")\n",
    "\n",
    "print(f\"\\n5. IMPACTO POTENCIAL\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "if 'churn_rate' in locals() and melhor_performance:\n",
    "    current_churn = churn_rate / 100\n",
    "    model_precision = melhor_performance.get('precision', 0.5)\n",
    "    model_recall = melhor_performance.get('recall', 0.5)\n",
    "    \n",
    "    print(f\"Cenário atual:\")\n",
    "    print(f\"  - Taxa de churn: {churn_rate:.1f}%\")\n",
    "    print(f\"  - Precisão do modelo: {model_precision:.1%}\")\n",
    "    print(f\"  - Recall do modelo: {model_recall:.1%}\")\n",
    "    \n",
    "    potential_reduction = 0.3  # Assumindo 30% de efetividade das ações de retenção\n",
    "    print(f\"\\nCenário otimista (30% de efetividade nas ações):\")\n",
    "    new_churn_rate = current_churn * (1 - model_recall * potential_reduction)\n",
    "    reduction = (current_churn - new_churn_rate) / current_churn * 100\n",
    "    print(f\"  - Nova taxa de churn estimada: {new_churn_rate*100:.1f}%\")\n",
    "    print(f\"  - Redução potencial: {reduction:.1f}%\")\n",
    "\n",
    "print(f\"\\n6. PRÓXIMOS PASSOS\")\n",
    "print(\"-\" * 18)\n",
    "print(\"1. Validar modelo em ambiente de produção\")\n",
    "print(\"2. Implementar sistema de scoring automático\")  \n",
    "print(\"3. Desenvolver campanhas específicas por segmento\")\n",
    "print(\"4. Criar dashboards de monitoramento\")\n",
    "print(\"5. Treinar equipes comerciais com insights dos modelos\")\n",
    "print(\"6. Estabelecer métricas de sucesso e ROI\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RELATÓRIO CONCLUÍDO\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c257256",
   "metadata": {},
   "source": [
    "## 2.5. Análise de Correlação e Seleção de Variáveis\n",
    "\n",
    "### Objetivos desta Análise:\n",
    "1. **Matriz de Correlação**: Identificar relações lineares entre variáveis numéricas\n",
    "2. **Correlação com Target**: Descobrir quais variáveis têm maior correlação com churn\n",
    "3. **Análises Específicas**:\n",
    "   - Tempo de contrato × Evasão\n",
    "   - Total gasto × Evasão\n",
    "   - Outros padrões relevantes\n",
    "4. **Visualizações Interativas**: Boxplots e scatter plots para insights visuais\n",
    "5. **Seleção de Features**: Identificar variáveis mais relevantes para modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636ae172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise 1: Matriz de Correlação Geral\n",
    "\n",
    "print(\"Análise de correlação - variáveis numéricas\")\n",
    "\n",
    "try:\n",
    "    dados_para_correlacao = pd.concat([X_final, pd.Series(y_encoded, name='Churn')], axis=1)\n",
    "    print(\"Usando dados pré-processados para análise de correlação\")\n",
    "except:\n",
    "    dados_para_correlacao = dados.copy()\n",
    "    print(\"Usando dados originais - execute as células de pré-processamento primeiro\")\n",
    "\n",
    "dados_numericos = dados_para_correlacao.select_dtypes(include=[np.number])\n",
    "\n",
    "print(f\"\\nVariáveis numéricas identificadas: {len(dados_numericos.columns)}\")\n",
    "for i, col in enumerate(dados_numericos.columns):\n",
    "    print(f\"{i+1:2d}. {col}\")\n",
    "\n",
    "matriz_correlacao = dados_numericos.corr()\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "mask = np.triu(np.ones_like(matriz_correlacao, dtype=bool))\n",
    "sns.heatmap(matriz_correlacao, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            fmt='.3f', \n",
    "            center=0,\n",
    "            cmap='RdBu_r', \n",
    "            square=True,\n",
    "            cbar_kws={'label': 'Coeficiente de Correlação'})\n",
    "\n",
    "plt.title('Matriz de Correlação - Variáveis Numéricas', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nEstatísticas da matriz de correlação:\")\n",
    "print(f\"   • Dimensões: {matriz_correlacao.shape}\")\n",
    "print(f\"   • Correlação máxima (excl. diagonal): {matriz_correlacao.where(~np.eye(len(matriz_correlacao), dtype=bool)).max().max():.3f}\")\n",
    "print(f\"   • Correlação mínima: {matriz_correlacao.where(~np.eye(len(matriz_correlacao), dtype=bool)).min().min():.3f}\")\n",
    "\n",
    "correlacoes_fortes = []\n",
    "for i in range(len(matriz_correlacao.columns)):\n",
    "    for j in range(i+1, len(matriz_correlacao.columns)):\n",
    "        corr_val = matriz_correlacao.iloc[i, j]\n",
    "        if abs(corr_val) > 0.5:\n",
    "            correlacoes_fortes.append({\n",
    "                'var1': matriz_correlacao.columns[i],\n",
    "                'var2': matriz_correlacao.columns[j],\n",
    "                'correlacao': corr_val\n",
    "            })\n",
    "\n",
    "if correlacoes_fortes:\n",
    "    print(f\"\\nCorrelações fortes (|r| > 0.5):\")\n",
    "    correlacoes_fortes_df = pd.DataFrame(correlacoes_fortes).sort_values('correlacao', key=abs, ascending=False)\n",
    "    for _, row in correlacoes_fortes_df.iterrows():\n",
    "        intensidade = \"MUITO FORTE\" if abs(row['correlacao']) > 0.8 else \"FORTE\" if abs(row['correlacao']) > 0.7 else \"MODERADA\"\n",
    "        print(f\"   {intensidade}: {row['var1']} ↔ {row['var2']}: {row['correlacao']:+.3f}\")\n",
    "else:\n",
    "    print(f\"\\nNenhuma correlação muito forte encontrada entre variáveis\")\n",
    "\n",
    "print(f\"\\nAnálise de correlação geral concluída\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise 2: Correlação com Variável Target (Churn)\n",
    "\n",
    "print(\"Correlação das variáveis com churn (evasão)\")\n",
    "\n",
    "coluna_churn = None\n",
    "for col in dados_numericos.columns:\n",
    "    if 'churn' in col.lower() or 'evasao' in col.lower():\n",
    "        coluna_churn = col\n",
    "        break\n",
    "\n",
    "if coluna_churn is None:\n",
    "    print(\"Coluna de churn não encontrada automaticamente\")\n",
    "    if 'Churn' in dados_numericos.columns:\n",
    "        coluna_churn = 'Churn'\n",
    "    else:\n",
    "        coluna_churn = dados_numericos.columns[-1]\n",
    "    print(f\"Assumindo '{coluna_churn}' como variável target\")\n",
    "\n",
    "print(f\"Variável target identificada: '{coluna_churn}'\")\n",
    "\n",
    "correlacoes_churn = dados_numericos.corr()[coluna_churn].drop(coluna_churn).sort_values(key=abs, ascending=False)\n",
    "\n",
    "print(f\"\\nCorrelações com {coluna_churn.upper()}:\")\n",
    "\n",
    "for i, (variavel, correlacao) in enumerate(correlacoes_churn.items()):\n",
    "    if abs(correlacao) > 0.7:\n",
    "        intensidade = \"MUITO FORTE\"\n",
    "    elif abs(correlacao) > 0.5:\n",
    "        intensidade = \"FORTE\"\n",
    "    elif abs(correlacao) > 0.3:\n",
    "        intensidade = \"MODERADA\"\n",
    "    elif abs(correlacao) > 0.1:\n",
    "        intensidade = \"FRACA\"\n",
    "    else:\n",
    "        intensidade = \"MUITO FRACA\"\n",
    "    \n",
    "    direcao = \"POSITIVA\" if correlacao > 0 else \"NEGATIVA\"\n",
    "    \n",
    "    print(f\"{i+1:2d}. {variavel:25s} | {correlacao:+.4f} | {intensidade} {direcao}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "correlacoes_churn_abs = correlacoes_churn.reindex(correlacoes_churn.abs().sort_values(ascending=True).index)\n",
    "colors = ['red' if x < 0 else 'blue' for x in correlacoes_churn_abs.values]\n",
    "\n",
    "axes[0].barh(range(len(correlacoes_churn_abs)), correlacoes_churn_abs.values, color=colors, alpha=0.7)\n",
    "axes[0].set_yticks(range(len(correlacoes_churn_abs)))\n",
    "axes[0].set_yticklabels(correlacoes_churn_abs.index, fontsize=10)\n",
    "axes[0].set_xlabel('Coeficiente de Correlação')\n",
    "axes[0].set_title(f'Correlação das Variáveis com {coluna_churn}', fontsize=12, fontweight='bold')\n",
    "axes[0].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "correlacoes_abs_top10 = correlacoes_churn.abs().sort_values(ascending=False).head(10)\n",
    "axes[1].bar(range(len(correlacoes_abs_top10)), correlacoes_abs_top10.values, \n",
    "           color='coral', alpha=0.7)\n",
    "axes[1].set_xticks(range(len(correlacoes_abs_top10)))\n",
    "axes[1].set_xticklabels(correlacoes_abs_top10.index, rotation=45, ha='right')\n",
    "axes[1].set_ylabel('Correlação Absoluta')\n",
    "axes[1].set_title(f'Top 10 Variáveis - Correlação Absoluta com {coluna_churn}', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "top_features_correlacao = correlacoes_churn.abs().sort_values(ascending=False).head(10)\n",
    "\n",
    "print(f\"\\nTop 10 variáveis mais correlacionadas com {coluna_churn.upper()}:\")\n",
    "for i, (var, corr_abs) in enumerate(top_features_correlacao.items()):\n",
    "    corr_original = correlacoes_churn[var]\n",
    "    print(f\"{i+1:2d}. {var:25s} | Correlação: {corr_original:+.4f} | |r|: {corr_abs:.4f}\")\n",
    "\n",
    "print(f\"\\nInsights para modelagem:\")\n",
    "features_relevantes = top_features_correlacao[top_features_correlacao > 0.1]\n",
    "print(f\"   • {len(features_relevantes)} variáveis têm correlação relevante (|r| > 0.1) com {coluna_churn}\")\n",
    "features_fortes = top_features_correlacao[top_features_correlacao > 0.3]\n",
    "print(f\"   • {len(features_fortes)} variáveis têm correlação moderada/forte (|r| > 0.3) com {coluna_churn}\")\n",
    "\n",
    "if len(features_fortes) > 0:\n",
    "    print(f\"   • Features principais: {list(features_fortes.index)}\")\n",
    "\n",
    "print(f\"\\nAnálise de correlação com target concluída\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1c7be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ⏰ Análise 3: Tempo de Contrato × Evasão\n",
    "\n",
    "print(\"⏰ ANÁLISE: TEMPO DE CONTRATO × EVASÃO\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Identificar coluna relacionada ao tempo de contrato\n",
    "coluna_tempo = None\n",
    "possíveis_colunas_tempo = ['tenure', 'tempo', 'contract', 'tempo_contrato', 'customer.tenure']\n",
    "\n",
    "for col in dados_para_correlacao.columns:\n",
    "    col_lower = col.lower()\n",
    "    if any(termo in col_lower for termo in ['tenure', 'tempo', 'contract']):\n",
    "        coluna_tempo = col\n",
    "        break\n",
    "\n",
    "if coluna_tempo is None:\n",
    "    print(\"⚠️ Coluna de tempo de contrato não encontrada automaticamente\")\n",
    "    print(\"📋 Colunas disponíveis:\")\n",
    "    for i, col in enumerate(dados_para_correlacao.columns):\n",
    "        print(f\"  {i+1:2d}. {col}\")\n",
    "    print(\"💡 Assumindo que existe uma variável relacionada ao tempo...\")\n",
    "    # Tentar encontrar qualquer coluna que possa representar tempo\n",
    "    for col in dados_para_correlacao.columns:\n",
    "        if dados_para_correlacao[col].dtype in ['int64', 'float64']:\n",
    "            # Verificar se os valores fazem sentido para tempo (ex: 0-100 meses)\n",
    "            if dados_para_correlacao[col].min() >= 0 and dados_para_correlacao[col].max() <= 200:\n",
    "                coluna_tempo = col\n",
    "                break\n",
    "    \n",
    "    if coluna_tempo:\n",
    "        print(f\"🔍 Assumindo '{coluna_tempo}' como variável de tempo\")\n",
    "    else:\n",
    "        print(\"❌ Não foi possível identificar variável de tempo\")\n",
    "\n",
    "if coluna_tempo and coluna_churn:\n",
    "    print(f\"\\n📊 ANALISANDO: {coluna_tempo} × {coluna_churn}\")\n",
    "    \n",
    "    # Estatísticas básicas por grupo de churn\n",
    "    stats_tempo = dados_para_correlacao.groupby(coluna_churn)[coluna_tempo].agg([\n",
    "        'count', 'mean', 'median', 'std', 'min', 'max'\n",
    "    ]).round(2)\n",
    "    \n",
    "    print(f\"\\n📈 ESTATÍSTICAS DE {coluna_tempo.upper()} POR GRUPO DE CHURN:\")\n",
    "    print(\"-\" * 55)\n",
    "    print(stats_tempo)\n",
    "    \n",
    "    # Calcular correlação específica\n",
    "    correlacao_tempo_churn = dados_para_correlacao[coluna_tempo].corr(dados_para_correlacao[coluna_churn])\n",
    "    print(f\"\\n🔗 CORRELAÇÃO {coluna_tempo} × {coluna_churn}: {correlacao_tempo_churn:+.4f}\")\n",
    "    \n",
    "    # Interpretação da correlação\n",
    "    if abs(correlacao_tempo_churn) > 0.5:\n",
    "        interpretacao = \"FORTE\"\n",
    "    elif abs(correlacao_tempo_churn) > 0.3:\n",
    "        interpretacao = \"MODERADA\"\n",
    "    elif abs(correlacao_tempo_churn) > 0.1:\n",
    "        interpretacao = \"FRACA\"\n",
    "    else:\n",
    "        interpretacao = \"MUITO FRACA\"\n",
    "    \n",
    "    direcao = \"NEGATIVA (↘️)\" if correlacao_tempo_churn < 0 else \"POSITIVA (↗️)\"\n",
    "    print(f\"💡 Interpretação: Correlação {interpretacao} {direcao}\")\n",
    "    \n",
    "    # Visualizações\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Boxplot - Tempo de contrato por grupo de churn\n",
    "    dados_para_correlacao.boxplot(column=coluna_tempo, by=coluna_churn, ax=axes[0,0])\n",
    "    axes[0,0].set_title(f'Boxplot: {coluna_tempo} por {coluna_churn}')\n",
    "    axes[0,0].set_xlabel(f'{coluna_churn} (0=Não Cancelou, 1=Cancelou)')\n",
    "    axes[0,0].set_ylabel(coluna_tempo)\n",
    "    \n",
    "    # 2. Histograma comparativo\n",
    "    churn_0 = dados_para_correlacao[dados_para_correlacao[coluna_churn] == 0][coluna_tempo]\n",
    "    churn_1 = dados_para_correlacao[dados_para_correlacao[coluna_churn] == 1][coluna_tempo]\n",
    "    \n",
    "    axes[0,1].hist(churn_0, bins=30, alpha=0.7, label='Não Cancelou', color='lightblue', density=True)\n",
    "    axes[0,1].hist(churn_1, bins=30, alpha=0.7, label='Cancelou', color='lightcoral', density=True)\n",
    "    axes[0,1].set_xlabel(coluna_tempo)\n",
    "    axes[0,1].set_ylabel('Densidade')\n",
    "    axes[0,1].set_title(f'Distribuição de {coluna_tempo} por Grupo')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(alpha=0.3)\n",
    "    \n",
    "    # 3. Scatter plot\n",
    "    cores = ['lightblue' if x == 0 else 'lightcoral' for x in dados_para_correlacao[coluna_churn]]\n",
    "    axes[1,0].scatter(dados_para_correlacao[coluna_tempo], dados_para_correlacao[coluna_churn], \n",
    "                     c=cores, alpha=0.6)\n",
    "    axes[1,0].set_xlabel(coluna_tempo)\n",
    "    axes[1,0].set_ylabel(coluna_churn)\n",
    "    axes[1,0].set_title(f'Scatter Plot: {coluna_tempo} × {coluna_churn}')\n",
    "    axes[1,0].grid(alpha=0.3)\n",
    "    \n",
    "    # 4. Violin plot\n",
    "    dados_violin = [churn_0.values, churn_1.values]\n",
    "    parts = axes[1,1].violinplot(dados_violin, positions=[0, 1], showmeans=True, showmedians=True)\n",
    "    axes[1,1].set_xticks([0, 1])\n",
    "    axes[1,1].set_xticklabels(['Não Cancelou', 'Cancelou'])\n",
    "    axes[1,1].set_ylabel(coluna_tempo)\n",
    "    axes[1,1].set_title(f'Violin Plot: {coluna_tempo} por {coluna_churn}')\n",
    "    axes[1,1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Análise de segmentação por quartis de tempo\n",
    "    quartis = dados_para_correlacao[coluna_tempo].quantile([0.25, 0.5, 0.75])\n",
    "    print(f\"\\n📊 ANÁLISE POR QUARTIS DE {coluna_tempo.upper()}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    def classificar_quartil(valor):\n",
    "        if valor <= quartis[0.25]:\n",
    "            return 'Q1 (Baixo)'\n",
    "        elif valor <= quartis[0.5]:\n",
    "            return 'Q2 (Médio-Baixo)'\n",
    "        elif valor <= quartis[0.75]:\n",
    "            return 'Q3 (Médio-Alto)'\n",
    "        else:\n",
    "            return 'Q4 (Alto)'\n",
    "    \n",
    "    dados_para_correlacao['Quartil_Tempo'] = dados_para_correlacao[coluna_tempo].apply(classificar_quartil)\n",
    "    quartil_analysis = dados_para_correlacao.groupby('Quartil_Tempo')[coluna_churn].agg(['count', 'sum', 'mean'])\n",
    "    quartil_analysis['taxa_churn_pct'] = (quartil_analysis['mean'] * 100).round(1)\n",
    "    \n",
    "    print(quartil_analysis)\n",
    "    \n",
    "    print(f\"\\n💡 INSIGHTS - {coluna_tempo.upper()} × EVASÃO:\")\n",
    "    q1_churn = quartil_analysis.loc['Q1 (Baixo)', 'taxa_churn_pct']\n",
    "    q4_churn = quartil_analysis.loc['Q4 (Alto)', 'taxa_churn_pct']\n",
    "    \n",
    "    if q1_churn > q4_churn:\n",
    "        print(f\"   • Clientes com MENOR {coluna_tempo} têm MAIOR taxa de evasão ({q1_churn}% vs {q4_churn}%)\")\n",
    "        print(f\"   • Retenção melhora com o aumento do tempo de relacionamento\")\n",
    "    else:\n",
    "        print(f\"   • Clientes com MAIOR {coluna_tempo} têm MAIOR taxa de evasão ({q4_churn}% vs {q1_churn}%)\")\n",
    "        print(f\"   • Padrão atípico - investigar possíveis causas\")\n",
    "    \n",
    "    print(f\"   • Diferença entre Q1 e Q4: {abs(q1_churn - q4_churn):.1f} pontos percentuais\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Não foi possível realizar análise de tempo × evasão\")\n",
    "    print(\"💡 Verifique se as colunas de tempo e churn estão disponíveis\")\n",
    "\n",
    "print(f\"\\n✅ Análise tempo de contrato × evasão concluída!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67937a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 💰 Análise 4: Total Gasto × Evasão\n",
    "\n",
    "print(\"💰 ANÁLISE: TOTAL GASTO × EVASÃO\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Identificar coluna relacionada ao gasto total\n",
    "coluna_gasto = None\n",
    "possíveis_colunas_gasto = ['total', 'charges', 'gasto', 'valor', 'account.Charges.Total']\n",
    "\n",
    "for col in dados_para_correlacao.columns:\n",
    "    col_lower = col.lower()\n",
    "    if any(termo in col_lower for termo in ['total', 'charges', 'gasto', 'valor']):\n",
    "        if 'total' in col_lower:  # Priorizar variáveis com \"total\"\n",
    "            coluna_gasto = col\n",
    "            break\n",
    "        elif coluna_gasto is None:  # Se não encontrou \"total\", aceitar outras\n",
    "            coluna_gasto = col\n",
    "\n",
    "if coluna_gasto is None:\n",
    "    print(\"⚠️ Coluna de gasto total não encontrada automaticamente\")\n",
    "    print(\"🔍 Procurando por colunas numéricas com valores altos (possível gasto)...\")\n",
    "    \n",
    "    for col in dados_para_correlacao.select_dtypes(include=[np.number]).columns:\n",
    "        if col != coluna_churn:  # Não é a variável target\n",
    "            max_val = dados_para_correlacao[col].max()\n",
    "            if max_val > 1000:  # Assumir que gastos são valores altos\n",
    "                coluna_gasto = col\n",
    "                print(f\"💡 Assumindo '{col}' como variável de gasto (max: {max_val:,.2f})\")\n",
    "                break\n",
    "\n",
    "if coluna_gasto and coluna_churn:\n",
    "    print(f\"\\n📊 ANALISANDO: {coluna_gasto} × {coluna_churn}\")\n",
    "    \n",
    "    # Remover outliers extremos para melhor visualização\n",
    "    Q1 = dados_para_correlacao[coluna_gasto].quantile(0.25)\n",
    "    Q3 = dados_para_correlacao[coluna_gasto].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    limite_inferior = Q1 - 1.5 * IQR\n",
    "    limite_superior = Q3 + 1.5 * IQR\n",
    "    \n",
    "    dados_sem_outliers = dados_para_correlacao[\n",
    "        (dados_para_correlacao[coluna_gasto] >= limite_inferior) & \n",
    "        (dados_para_correlacao[coluna_gasto] <= limite_superior)\n",
    "    ]\n",
    "    \n",
    "    outliers_removidos = len(dados_para_correlacao) - len(dados_sem_outliers)\n",
    "    print(f\"📊 Outliers identificados e removidos para análise: {outliers_removidos} ({outliers_removidos/len(dados_para_correlacao)*100:.1f}%)\")\n",
    "    \n",
    "    # Estatísticas básicas por grupo de churn\n",
    "    stats_gasto = dados_para_correlacao.groupby(coluna_churn)[coluna_gasto].agg([\n",
    "        'count', 'mean', 'median', 'std', 'min', 'max'\n",
    "    ]).round(2)\n",
    "    \n",
    "    print(f\"\\n📈 ESTATÍSTICAS DE {coluna_gasto.upper()} POR GRUPO DE CHURN:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(stats_gasto)\n",
    "    \n",
    "    # Calcular correlação específica\n",
    "    correlacao_gasto_churn = dados_para_correlacao[coluna_gasto].corr(dados_para_correlacao[coluna_churn])\n",
    "    print(f\"\\n🔗 CORRELAÇÃO {coluna_gasto} × {coluna_churn}: {correlacao_gasto_churn:+.4f}\")\n",
    "    \n",
    "    # Interpretação da correlação\n",
    "    if abs(correlacao_gasto_churn) > 0.5:\n",
    "        interpretacao = \"FORTE\"\n",
    "    elif abs(correlacao_gasto_churn) > 0.3:\n",
    "        interpretacao = \"MODERADA\"\n",
    "    elif abs(correlacao_gasto_churn) > 0.1:\n",
    "        interpretacao = \"FRACA\"\n",
    "    else:\n",
    "        interpretacao = \"MUITO FRACA\"\n",
    "    \n",
    "    direcao = \"NEGATIVA (↘️)\" if correlacao_gasto_churn < 0 else \"POSITIVA (↗️)\"\n",
    "    print(f\"💡 Interpretação: Correlação {interpretacao} {direcao}\")\n",
    "    \n",
    "    # Visualizações\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Boxplot - Gasto total por grupo de churn (sem outliers para melhor viz)\n",
    "    dados_sem_outliers.boxplot(column=coluna_gasto, by=coluna_churn, ax=axes[0,0])\n",
    "    axes[0,0].set_title(f'Boxplot: {coluna_gasto} por {coluna_churn}\\n(Outliers removidos para visualização)')\n",
    "    axes[0,0].set_xlabel(f'{coluna_churn} (0=Não Cancelou, 1=Cancelou)')\n",
    "    axes[0,0].set_ylabel(f'{coluna_gasto}')\n",
    "    \n",
    "    # 2. Histograma comparativo\n",
    "    churn_0_gasto = dados_para_correlacao[dados_para_correlacao[coluna_churn] == 0][coluna_gasto]\n",
    "    churn_1_gasto = dados_para_correlacao[dados_para_correlacao[coluna_churn] == 1][coluna_gasto]\n",
    "    \n",
    "    axes[0,1].hist(churn_0_gasto, bins=30, alpha=0.7, label='Não Cancelou', color='lightblue', density=True)\n",
    "    axes[0,1].hist(churn_1_gasto, bins=30, alpha=0.7, label='Cancelou', color='lightcoral', density=True)\n",
    "    axes[0,1].set_xlabel(coluna_gasto)\n",
    "    axes[0,1].set_ylabel('Densidade')\n",
    "    axes[0,1].set_title(f'Distribuição de {coluna_gasto} por Grupo')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(alpha=0.3)\n",
    "    \n",
    "    # 3. Scatter plot com jitter para visualizar densidade\n",
    "    y_jitter = dados_para_correlacao[coluna_churn] + np.random.normal(0, 0.05, len(dados_para_correlacao))\n",
    "    cores = ['lightblue' if x == 0 else 'lightcoral' for x in dados_para_correlacao[coluna_churn]]\n",
    "    axes[1,0].scatter(dados_para_correlacao[coluna_gasto], y_jitter, \n",
    "                     c=cores, alpha=0.6, s=20)\n",
    "    axes[1,0].set_xlabel(coluna_gasto)\n",
    "    axes[1,0].set_ylabel(f'{coluna_churn} (com jitter)')\n",
    "    axes[1,0].set_title(f'Scatter Plot: {coluna_gasto} × {coluna_churn}')\n",
    "    axes[1,0].set_ylim(-0.5, 1.5)\n",
    "    axes[1,0].grid(alpha=0.3)\n",
    "    \n",
    "    # 4. Violin plot (usando dados sem outliers)\n",
    "    dados_violin = [\n",
    "        dados_sem_outliers[dados_sem_outliers[coluna_churn] == 0][coluna_gasto].values,\n",
    "        dados_sem_outliers[dados_sem_outliers[coluna_churn] == 1][coluna_gasto].values\n",
    "    ]\n",
    "    parts = axes[1,1].violinplot(dados_violin, positions=[0, 1], showmeans=True, showmedians=True)\n",
    "    axes[1,1].set_xticks([0, 1])\n",
    "    axes[1,1].set_xticklabels(['Não Cancelou', 'Cancelou'])\n",
    "    axes[1,1].set_ylabel(coluna_gasto)\n",
    "    axes[1,1].set_title(f'Violin Plot: {coluna_gasto} por {coluna_churn}')\n",
    "    axes[1,1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Análise por faixas de gasto\n",
    "    # Criar faixas de gasto baseadas em quartis\n",
    "    quartis_gasto = dados_para_correlacao[coluna_gasto].quantile([0, 0.25, 0.5, 0.75, 1.0])\n",
    "    \n",
    "    print(f\"\\n📊 QUARTIS DE {coluna_gasto.upper()}:\")\n",
    "    for i, (q, valor) in enumerate(quartis_gasto.items()):\n",
    "        print(f\"   Q{int(q*4) if q > 0 else 'Min'}: R$ {valor:,.2f}\")\n",
    "    \n",
    "    def classificar_faixa_gasto(valor):\n",
    "        if valor <= quartis_gasto[0.25]:\n",
    "            return 'Baixo Gasto (Q1)'\n",
    "        elif valor <= quartis_gasto[0.5]:\n",
    "            return 'Gasto Moderado (Q2)'\n",
    "        elif valor <= quartis_gasto[0.75]:\n",
    "            return 'Alto Gasto (Q3)'\n",
    "        else:\n",
    "            return 'Muito Alto Gasto (Q4)'\n",
    "    \n",
    "    dados_para_correlacao['Faixa_Gasto'] = dados_para_correlacao[coluna_gasto].apply(classificar_faixa_gasto)\n",
    "    faixa_analysis = dados_para_correlacao.groupby('Faixa_Gasto')[coluna_churn].agg(['count', 'sum', 'mean'])\n",
    "    faixa_analysis['taxa_churn_pct'] = (faixa_analysis['mean'] * 100).round(1)\n",
    "    \n",
    "    print(f\"\\n📈 TAXA DE EVASÃO POR FAIXA DE GASTO:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(faixa_analysis[['count', 'taxa_churn_pct']].sort_values('taxa_churn_pct', ascending=False))\n",
    "    \n",
    "    # Insights específicos\n",
    "    print(f\"\\n💡 INSIGHTS - {coluna_gasto.upper()} × EVASÃO:\")\n",
    "    \n",
    "    # Comparar diferenças entre grupos\n",
    "    media_gasto_nao_churn = stats_gasto.loc[0, 'mean']\n",
    "    media_gasto_churn = stats_gasto.loc[1, 'mean']\n",
    "    diferenca_absoluta = abs(media_gasto_churn - media_gasto_nao_churn)\n",
    "    diferenca_percentual = (diferenca_absoluta / media_gasto_nao_churn) * 100\n",
    "    \n",
    "    if media_gasto_churn > media_gasto_nao_churn:\n",
    "        print(f\"   • Clientes que CANCELARAM gastam MAIS em média (R$ {media_gasto_churn:,.2f} vs R$ {media_gasto_nao_churn:,.2f})\")\n",
    "        print(f\"   • Diferença de R$ {diferenca_absoluta:,.2f} ({diferenca_percentual:.1f}% a mais)\")\n",
    "        print(f\"   • Possível insatisfação com custo-benefício ou busca por alternativas mais baratas\")\n",
    "    else:\n",
    "        print(f\"   • Clientes que CANCELARAM gastam MENOS em média (R$ {media_gasto_churn:,.2f} vs R$ {media_gasto_nao_churn:,.2f})\")\n",
    "        print(f\"   • Diferença de R$ {diferenca_absoluta:,.2f} ({diferenca_percentual:.1f}% a menos)\")\n",
    "        print(f\"   • Possível baixo engajamento ou uso limitado dos serviços\")\n",
    "    \n",
    "    # Análise das faixas\n",
    "    maior_evasao_faixa = faixa_analysis['taxa_churn_pct'].idxmax()\n",
    "    menor_evasao_faixa = faixa_analysis['taxa_churn_pct'].idxmin()\n",
    "    \n",
    "    print(f\"   • Maior taxa de evasão: {maior_evasao_faixa} ({faixa_analysis.loc[maior_evasao_faixa, 'taxa_churn_pct']:.1f}%)\")\n",
    "    print(f\"   • Menor taxa de evasão: {menor_evasao_faixa} ({faixa_analysis.loc[menor_evasao_faixa, 'taxa_churn_pct']:.1f}%)\")\n",
    "    \n",
    "    variacao_faixas = faixa_analysis['taxa_churn_pct'].max() - faixa_analysis['taxa_churn_pct'].min()\n",
    "    print(f\"   • Variação entre faixas: {variacao_faixas:.1f} pontos percentuais\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Não foi possível realizar análise de gasto × evasão\")\n",
    "    print(\"💡 Verifique se as colunas de gasto e churn estão disponíveis\")\n",
    "\n",
    "print(f\"\\n✅ Análise total gasto × evasão concluída!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18743389",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 🎯 Análise 5: Seleção Final de Variáveis\n",
    "\n",
    "print(\"🎯 SELEÇÃO FINAL DE VARIÁVEIS PARA MODELAGEM\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Compilar todas as análises de correlação realizadas\n",
    "print(\"📊 RESUMO DAS ANÁLISES DE CORRELAÇÃO:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'correlacoes_churn' in locals():\n",
    "    # Classificar variáveis por importância baseada na correlação absoluta\n",
    "    ranking_features = correlacoes_churn.abs().sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"🏆 RANKING COMPLETO - CORRELAÇÃO ABSOLUTA COM {coluna_churn}:\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    features_selecionadas = {'muito_importantes': [], 'importantes': [], 'moderadas': [], 'fracas': []}\n",
    "    \n",
    "    for i, (feature, corr_abs) in enumerate(ranking_features.items()):\n",
    "        corr_original = correlacoes_churn[feature]\n",
    "        \n",
    "        # Classificação por nível de importância\n",
    "        if corr_abs >= 0.5:\n",
    "            categoria = 'muito_importantes'\n",
    "            emoji = '🔴'\n",
    "            nivel = 'MUITO IMPORTANTE'\n",
    "        elif corr_abs >= 0.3:\n",
    "            categoria = 'importantes'\n",
    "            emoji = '🟠'\n",
    "            nivel = 'IMPORTANTE'\n",
    "        elif corr_abs >= 0.1:\n",
    "            categoria = 'moderadas'\n",
    "            emoji = '🟡'\n",
    "            nivel = 'MODERADA'\n",
    "        else:\n",
    "            categoria = 'fracas'\n",
    "            emoji = '🟢'\n",
    "            nivel = 'FRACA'\n",
    "        \n",
    "        features_selecionadas[categoria].append(feature)\n",
    "        \n",
    "        print(f\"{i+1:2d}. {emoji} {feature:25s} | r={corr_original:+.4f} | |r|={corr_abs:.4f} | {nivel}\")\n",
    "    \n",
    "    # Resumo por categoria\n",
    "    print(f\"\\n📈 RESUMO POR CATEGORIA DE IMPORTÂNCIA:\")\n",
    "    print(\"-\" * 45)\n",
    "    for categoria, features in features_selecionadas.items():\n",
    "        if features:\n",
    "            print(f\"{categoria.replace('_', ' ').title():20s}: {len(features):2d} variáveis - {features}\")\n",
    "    \n",
    "    # Recomendações para seleção\n",
    "    total_muito_importantes = len(features_selecionadas['muito_importantes'])\n",
    "    total_importantes = len(features_selecionadas['importantes'])\n",
    "    total_moderadas = len(features_selecionadas['moderadas'])\n",
    "    \n",
    "    print(f\"\\n🎯 RECOMENDAÇÕES PARA SELEÇÃO DE FEATURES:\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    if total_muito_importantes > 0:\n",
    "        print(f\"✅ OBRIGATÓRIAS ({total_muito_importantes}): {features_selecionadas['muito_importantes']}\")\n",
    "    \n",
    "    if total_importantes > 0:\n",
    "        print(f\"🔶 ALTAMENTE RECOMENDADAS ({total_importantes}): {features_selecionadas['importantes']}\")\n",
    "    \n",
    "    if total_moderadas > 0:\n",
    "        print(f\"🟡 CONSIDERAR INCLUIR ({total_moderadas}): {features_selecionadas['moderadas'][:5]}\")  # Top 5\n",
    "        if len(features_selecionadas['moderadas']) > 5:\n",
    "            print(f\"   ... e mais {len(features_selecionadas['moderadas'])-5} variáveis moderadas\")\n",
    "    \n",
    "    # Feature set recomendado\n",
    "    features_recomendadas = (features_selecionadas['muito_importantes'] + \n",
    "                           features_selecionadas['importantes'] + \n",
    "                           features_selecionadas['moderadas'][:10])  # Top 10 moderadas\n",
    "    \n",
    "    print(f\"\\n🏆 CONJUNTO FINAL RECOMENDADO ({len(features_recomendadas)} variáveis):\")\n",
    "    print(\"-\" * 55)\n",
    "    for i, feature in enumerate(features_recomendadas, 1):\n",
    "        corr_val = correlacoes_churn[feature]\n",
    "        print(f\"{i:2d}. {feature:25s} (r={corr_val:+.4f})\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Execute a análise de correlação com target primeiro\")\n",
    "\n",
    "# Análise de multicolinearidade entre features selecionadas\n",
    "if 'features_recomendadas' in locals() and len(features_recomendadas) > 1:\n",
    "    print(f\"\\n🔍 ANÁLISE DE MULTICOLINEARIDADE - FEATURES SELECIONADAS:\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    # Matriz de correlação apenas das features selecionadas\n",
    "    features_disponiveis = [f for f in features_recomendadas if f in dados_numericos.columns]\n",
    "    \n",
    "    if len(features_disponiveis) > 1:\n",
    "        matriz_corr_selecionadas = dados_numericos[features_disponiveis].corr()\n",
    "        \n",
    "        # Identificar pares com alta correlação (multicolinearidade)\n",
    "        pares_correlacionados = []\n",
    "        for i in range(len(matriz_corr_selecionadas.columns)):\n",
    "            for j in range(i+1, len(matriz_corr_selecionadas.columns)):\n",
    "                corr_val = matriz_corr_selecionadas.iloc[i, j]\n",
    "                if abs(corr_val) > 0.7:  # Alta correlação entre features\n",
    "                    pares_correlacionados.append({\n",
    "                        'var1': matriz_corr_selecionadas.columns[i],\n",
    "                        'var2': matriz_corr_selecionadas.columns[j],\n",
    "                        'correlacao': corr_val\n",
    "                    })\n",
    "        \n",
    "        if pares_correlacionados:\n",
    "            print(\"⚠️  ALTA CORRELAÇÃO ENTRE FEATURES (|r| > 0.7):\")\n",
    "            for par in pares_correlacionados:\n",
    "                print(f\"   🔴 {par['var1']} ↔ {par['var2']}: {par['correlacao']:+.3f}\")\n",
    "            print(\"💡 Considere remover uma das variáveis de cada par para evitar redundância\")\n",
    "        else:\n",
    "            print(\"✅ Não há multicolinearidade significativa entre features selecionadas\")\n",
    "        \n",
    "        # Visualização da matriz de correlação das features selecionadas\n",
    "        if len(features_disponiveis) <= 15:  # Só plotar se não for muito grande\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            mask = np.triu(np.ones_like(matriz_corr_selecionadas, dtype=bool))\n",
    "            sns.heatmap(matriz_corr_selecionadas, \n",
    "                       mask=mask,\n",
    "                       annot=True, \n",
    "                       fmt='.3f', \n",
    "                       center=0,\n",
    "                       cmap='RdBu_r', \n",
    "                       square=True,\n",
    "                       cbar_kws={'label': 'Correlação'})\n",
    "            plt.title('Matriz de Correlação - Features Selecionadas', fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Salvar informações para próximas etapas\n",
    "if 'features_recomendadas' in locals():\n",
    "    print(f\"\\n💾 VARIÁVEIS CRIADAS PARA PRÓXIMAS ETAPAS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"• features_recomendadas: Lista com {len(features_recomendadas)} variáveis selecionadas\")\n",
    "    print(f\"• ranking_features: Ranking completo de importância\")\n",
    "    if 'pares_correlacionados' in locals():\n",
    "        print(f\"• pares_correlacionados: {len(pares_correlacionados)} pares com alta correlação\")\n",
    "    \n",
    "    print(f\"\\n✅ Análise de seleção de variáveis concluída!\")\n",
    "    print(f\"🎯 {len(features_recomendadas)} variáveis selecionadas para modelagem\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Execute as análises de correlação primeiro para gerar as recomendações\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 ANÁLISE COMPLETA DE CORRELAÇÃO E SELEÇÃO DE VARIÁVEIS FINALIZADA!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6bd8f1",
   "metadata": {},
   "source": [
    "## 🔧 2. Preparação dos Dados para Machine Learning\n",
    "\n",
    "Nesta etapa, vamos preparar os dados para aplicar os algoritmos de Machine Learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar features (variáveis independentes) e target (variável dependente)\n",
    "\n",
    "# Remover colunas que não são úteis para o modelo\n",
    "colunas_remover = ['customerID']  # ID não é predictivo\n",
    "\n",
    "# Identificar colunas categóricas e numéricas\n",
    "colunas_categoricas = dados.select_dtypes(include=['object']).columns.tolist()\n",
    "colunas_numericas = dados.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Remover Churn das listas (é nossa variável target)\n",
    "if 'Churn' in colunas_categoricas:\n",
    "    colunas_categoricas.remove('Churn')\n",
    "if 'Churn' in colunas_numericas:\n",
    "    colunas_numericas.remove('Churn')\n",
    "\n",
    "print(f\"📊 Colunas categóricas: {len(colunas_categoricas)}\")\n",
    "print(colunas_categoricas)\n",
    "print(f\"\\n🔢 Colunas numéricas: {len(colunas_numericas)}\")\n",
    "print(colunas_numericas)\n",
    "\n",
    "# Preparar dataset final\n",
    "dados_ml = dados.copy()\n",
    "\n",
    "# Remover colunas desnecessárias\n",
    "for col in colunas_remover:\n",
    "    if col in dados_ml.columns:\n",
    "        dados_ml = dados_ml.drop(columns=[col])\n",
    "\n",
    "print(f\"\\n✅ Dataset preparado com {dados_ml.shape[1]} colunas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13327452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding de variáveis categóricas\n",
    "print(\"=== ENCODING DE VARIÁVEIS CATEGÓRICAS ===\")\n",
    "\n",
    "dados_encoded = dados_ml.copy()\n",
    "\n",
    "# Aplicar LabelEncoder para cada coluna categórica\n",
    "label_encoders = {}\n",
    "\n",
    "for col in colunas_categoricas:\n",
    "    if col in dados_encoded.columns:\n",
    "        le = LabelEncoder()\n",
    "        dados_encoded[col] = le.fit_transform(dados_encoded[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        \n",
    "        print(f\"✅ {col}: {len(le.classes_)} categorias únicas\")\n",
    "\n",
    "print(f\"\\n📊 Shape após encoding: {dados_encoded.shape}\")\n",
    "\n",
    "# Verificar se há valores ausentes após encoding\n",
    "missing_after_encoding = dados_encoded.isnull().sum().sum()\n",
    "print(f\"🔍 Valores ausentes após encoding: {missing_after_encoding}\")\n",
    "\n",
    "# Remover registros com valores ausentes se houver\n",
    "if missing_after_encoding > 0:\n",
    "    dados_encoded = dados_encoded.dropna()\n",
    "    print(f\"🧹 Shape após remoção de NaN: {dados_encoded.shape}\")\n",
    "\n",
    "dados_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d59a25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão dos dados em features (X) e target (y)\n",
    "X = dados_encoded.drop('Churn', axis=1)\n",
    "y = dados_encoded['Churn']\n",
    "\n",
    "print(f\"📊 Features (X): {X.shape}\")\n",
    "print(f\"🎯 Target (y): {y.shape}\")\n",
    "print(f\"📋 Colunas features: {X.columns.tolist()}\")\n",
    "\n",
    "# Divisão em treino e teste (80% treino, 20% teste)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # Manter proporção de classes\n",
    ")\n",
    "\n",
    "print(f\"\\n=== DIVISÃO DOS DADOS ===\")\n",
    "print(f\"🏋️ Treino - X: {X_train.shape}, y: {y_train.shape}\")\n",
    "print(f\"🧪 Teste - X: {X_test.shape}, y: {y_test.shape}\")\n",
    "\n",
    "# Verificar distribuição das classes no treino e teste\n",
    "print(f\"\\n📊 Distribuição no treino:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(f\"\\n📊 Distribuição no teste:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e074e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalização dos dados (StandardScaler)\n",
    "print(\"=== NORMALIZAÇÃO DOS DADOS ===\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit no conjunto de treino e transform em ambos\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"✅ Dados normalizados!\")\n",
    "print(f\"📊 Média antes da normalização (amostra): {X_train.mean().head()}\")\n",
    "print(f\"📊 Média após normalização (amostra): {X_train_scaled.mean(axis=0)[:5]}\")\n",
    "print(f\"📊 Desvio padrão após normalização (amostra): {X_train_scaled.std(axis=0)[:5]}\")\n",
    "\n",
    "# Converter de volta para DataFrame (opcional, para manter nomes das colunas)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97979b78",
   "metadata": {},
   "source": [
    "## 🤖 3. Construção dos Modelos Preditivos\n",
    "\n",
    "Vamos treinar e avaliar diferentes algoritmos de Machine Learning para prever o churn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ca785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir os modelos para comparação\n",
    "modelos = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "}\n",
    "\n",
    "# Dicionário para armazenar resultados\n",
    "resultados = {}\n",
    "\n",
    "print(\"🚀 TREINANDO MODELOS...\")\n",
    "\n",
    "for nome_modelo, modelo in modelos.items():\n",
    "    print(f\"\\n📊 Treinando: {nome_modelo}\")\n",
    "    \n",
    "    # Treinar o modelo\n",
    "    if nome_modelo == 'Logistic Regression':\n",
    "        # Logistic Regression funciona melhor com dados normalizados\n",
    "        modelo.fit(X_train_scaled, y_train)\n",
    "        y_pred = modelo.predict(X_test_scaled)\n",
    "        y_pred_proba = modelo.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        # Tree-based models não precisam de normalização\n",
    "        modelo.fit(X_train, y_train)\n",
    "        y_pred = modelo.predict(X_test)\n",
    "        y_pred_proba = modelo.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calcular métricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    resultados[nome_modelo] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'modelo': modelo\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ {nome_modelo} treinado!\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   F1-Score: {f1:.4f}\")\n",
    "    print(f\"   ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\n🎉 Todos os modelos foram treinados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc314ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparação dos modelos\n",
    "print(\"=\"*60)\n",
    "print(\"📊 COMPARAÇÃO DOS MODELOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Criar DataFrame com resultados\n",
    "df_resultados = pd.DataFrame(resultados).T\n",
    "df_metricas = df_resultados[['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']]\n",
    "\n",
    "print(df_metricas.round(4))\n",
    "\n",
    "# Identificar melhor modelo por métrica\n",
    "print(f\"\\n🏆 MELHORES MODELOS POR MÉTRICA:\")\n",
    "for metrica in df_metricas.columns:\n",
    "    melhor_modelo = df_metricas[metrica].idxmax()\n",
    "    melhor_score = df_metricas[metrica].max()\n",
    "    print(f\"   {metrica.upper()}: {melhor_modelo} ({melhor_score:.4f})\")\n",
    "\n",
    "# Visualização dos resultados\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "metricas = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
    "colors = ['skyblue', 'lightgreen', 'coral', 'gold', 'lightpink']\n",
    "\n",
    "for i, metrica in enumerate(metricas):\n",
    "    ax = axes[i]\n",
    "    df_metricas[metrica].plot(kind='bar', ax=ax, color=colors[i])\n",
    "    ax.set_title(f'{metrica.replace(\"_\", \" \").title()}')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_xlabel('Modelo')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Remover o subplot vazio\n",
    "axes[5].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea698463",
   "metadata": {},
   "source": [
    "## 4. Análise Detalhada do Melhor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaca69ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar melhor modelo baseado no F1-Score (boa métrica para dados desbalanceados)\n",
    "melhor_modelo_nome = df_metricas['f1_score'].idxmax()\n",
    "melhor_modelo_resultado = resultados[melhor_modelo_nome]\n",
    "\n",
    "print(f\"🏆 MELHOR MODELO: {melhor_modelo_nome}\")\n",
    "print(f\"📊 F1-Score: {melhor_modelo_resultado['f1_score']:.4f}\")\n",
    "\n",
    "# Matriz de Confusão\n",
    "y_pred_melhor = melhor_modelo_resultado['y_pred']\n",
    "cm = confusion_matrix(y_test, y_pred_melhor)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Matriz de Confusão\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Não Cancelou', 'Cancelou'],\n",
    "            yticklabels=['Não Cancelou', 'Cancelou'])\n",
    "axes[0].set_title(f'Matriz de Confusão - {melhor_modelo_nome}')\n",
    "axes[0].set_xlabel('Predição')\n",
    "axes[0].set_ylabel('Real')\n",
    "\n",
    "# Curva ROC\n",
    "y_pred_proba_melhor = melhor_modelo_resultado['y_pred_proba']\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_melhor)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "axes[1].plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('Taxa de Falsos Positivos')\n",
    "axes[1].set_ylabel('Taxa de Verdadeiros Positivos')\n",
    "axes[1].set_title('Curva ROC')\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Relatório detalhado\n",
    "print(f\"\\n📋 RELATÓRIO DETALHADO - {melhor_modelo_nome}\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, y_pred_melhor, \n",
    "                          target_names=['Não Cancelou', 'Cancelou']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee070a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de Importância das Features (se for Random Forest ou Decision Tree)\n",
    "if melhor_modelo_nome in ['Random Forest', 'Decision Tree']:\n",
    "    \n",
    "    modelo_treinado = melhor_modelo_resultado['modelo']\n",
    "    importancias = modelo_treinado.feature_importances_\n",
    "    \n",
    "    # Criar DataFrame com importâncias\n",
    "    df_importancias = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importancia': importancias\n",
    "    }).sort_values('importancia', ascending=False)\n",
    "    \n",
    "    # Plotar top 10 features mais importantes\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_10 = df_importancias.head(10)\n",
    "    \n",
    "    plt.barh(range(len(top_10)), top_10['importancia'], color='lightcoral')\n",
    "    plt.yticks(range(len(top_10)), top_10['feature'])\n",
    "    plt.xlabel('Importância')\n",
    "    plt.title(f'Top 10 Features Mais Importantes - {melhor_modelo_nome}')\n",
    "    plt.gca().invert_yaxis()  # Maior importância no topo\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"📊 TOP 10 FEATURES MAIS IMPORTANTES:\")\n",
    "    for i, row in top_10.iterrows():\n",
    "        print(f\"{row['feature']:30}: {row['importancia']:.4f}\")\n",
    "\n",
    "elif melhor_modelo_nome == 'Logistic Regression':\n",
    "    \n",
    "    modelo_treinado = melhor_modelo_resultado['modelo']\n",
    "    coeficientes = modelo_treinado.coef_[0]\n",
    "    \n",
    "    # Criar DataFrame com coeficientes (valores absolutos para ranking)\n",
    "    df_coeficientes = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'coeficiente': coeficientes,\n",
    "        'abs_coeficiente': np.abs(coeficientes)\n",
    "    }).sort_values('abs_coeficiente', ascending=False)\n",
    "    \n",
    "    # Plotar top 10 coeficientes mais importantes\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_10 = df_coeficientes.head(10)\n",
    "    \n",
    "    colors = ['red' if x < 0 else 'blue' for x in top_10['coeficiente']]\n",
    "    plt.barh(range(len(top_10)), top_10['coeficiente'], color=colors)\n",
    "    plt.yticks(range(len(top_10)), top_10['feature'])\n",
    "    plt.xlabel('Coeficiente')\n",
    "    plt.title(f'Top 10 Coeficientes Mais Importantes - {melhor_modelo_nome}')\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"📊 TOP 10 COEFICIENTES MAIS IMPORTANTES:\")\n",
    "    for i, row in top_10.iterrows():\n",
    "        sinal = \"📈\" if row['coeficiente'] > 0 else \"📉\"\n",
    "        print(f\"{row['feature']:30}: {row['coeficiente']:8.4f} {sinal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ac6042",
   "metadata": {},
   "source": [
    "## 5. Conclusões e Recomendações\n",
    "\n",
    "### Resultados Obtidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791dde48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"📊 RELATÓRIO FINAL - MODELOS PREDITIVOS TELECOM X\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Resumo dos resultados\n",
    "melhor_f1 = df_metricas['f1_score'].max()\n",
    "melhor_accuracy = df_metricas['accuracy'].max()\n",
    "melhor_roc_auc = df_metricas['roc_auc'].max()\n",
    "\n",
    "print(f\"\\n🎯 PERFORMANCE DOS MODELOS:\")\n",
    "print(f\"   • Melhor F1-Score: {melhor_f1:.4f} ({df_metricas['f1_score'].idxmax()})\")\n",
    "print(f\"   • Melhor Accuracy: {melhor_accuracy:.4f} ({df_metricas['accuracy'].idxmax()})\")\n",
    "print(f\"   • Melhor ROC-AUC: {melhor_roc_auc:.4f} ({df_metricas['roc_auc'].idxmax()})\")\n",
    "\n",
    "print(f\"\\n🏆 MODELO RECOMENDADO: {melhor_modelo_nome}\")\n",
    "print(f\"   • F1-Score: {melhor_modelo_resultado['f1_score']:.4f}\")\n",
    "print(f\"   • Accuracy: {melhor_modelo_resultado['accuracy']:.4f}\")\n",
    "print(f\"   • Precision: {melhor_modelo_resultado['precision']:.4f}\")\n",
    "print(f\"   • Recall: {melhor_modelo_resultado['recall']:.4f}\")\n",
    "print(f\"   • ROC-AUC: {melhor_modelo_resultado['roc_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\n💡 INSIGHTS PRINCIPAIS:\")\n",
    "print(f\"   • Taxa de churn no dataset: {(y.sum()/len(y)*100):.1f}%\")\n",
    "print(f\"   • O modelo consegue identificar {melhor_modelo_resultado['recall']:.1%} dos clientes que vão cancelar\")\n",
    "print(f\"   • Precisão de {melhor_modelo_resultado['precision']:.1%} nas predições de churn\")\n",
    "\n",
    "print(f\"\\n🎯 RECOMENDAÇÕES DE NEGÓCIO:\")\n",
    "print(f\"   1. Implementar sistema de scoring de churn usando o {melhor_modelo_nome}\")\n",
    "print(f\"   2. Focar nas features mais importantes identificadas pelo modelo\")\n",
    "print(f\"   3. Criar campanhas de retenção para clientes com alta probabilidade de churn\")\n",
    "print(f\"   4. Monitorar continuamente a performance do modelo\")\n",
    "print(f\"   5. Coletar feedback dos resultados para melhorar o modelo\")\n",
    "\n",
    "print(f\"\\n📈 PRÓXIMOS PASSOS:\")\n",
    "print(f\"   • Testar técnicas de balanceamento de classes (SMOTE, undersampling)\")\n",
    "print(f\"   • Realizar feature engineering para criar novas variáveis\")\n",
    "print(f\"   • Experimentar outros algoritmos (XGBoost, SVM)\")\n",
    "print(f\"   • Implementar validação cruzada mais robusta\")\n",
    "print(f\"   • Criar pipeline de deploy do modelo\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 ANÁLISE CONCLUÍDA COM SUCESSO!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
